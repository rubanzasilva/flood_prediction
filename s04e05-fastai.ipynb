{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":73278,"databundleVersionId":8121328,"sourceType":"competition"},{"sourceId":7277740,"sourceType":"datasetVersion","datasetId":4219453}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/rubanzasilva/s04e05-fastai?scriptVersionId=176529501\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Regression with a Flood Prediction Dataset\n\nPlayground Series - Season 4, Episode 5 where we are tasked with predicting the likelihood of floods in certain areas based off various factors.","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-05-08T16:09:53.714893Z","iopub.execute_input":"2024-05-08T16:09:53.715405Z","iopub.status.idle":"2024-05-08T16:09:53.780003Z","shell.execute_reply.started":"2024-05-08T16:09:53.715365Z","shell.execute_reply":"2024-05-08T16:09:53.778675Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/flood-prediction-factors/flood.csv\n/kaggle/input/playground-series-s4e5/sample_submission.csv\n/kaggle/input/playground-series-s4e5/train.csv\n/kaggle/input/playground-series-s4e5/test.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"#hide\n!pip install -Uqq fastbook\nimport fastbook\nfastbook.setup_book()\n\nfrom fastbook import *\n!pip install fastkaggle\n!pip install optuna\n!pip install optuna_distributed","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-05-08T16:09:53.782372Z","iopub.execute_input":"2024-05-08T16:09:53.782741Z","iopub.status.idle":"2024-05-08T16:11:15.945161Z","shell.execute_reply.started":"2024-05-08T16:09:53.78271Z","shell.execute_reply":"2024-05-08T16:11:15.943231Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Collecting fastkaggle\n  Downloading fastkaggle-0.0.8-py3-none-any.whl.metadata (4.3 kB)\nRequirement already satisfied: fastcore>=1.4.5 in /opt/conda/lib/python3.10/site-packages (from fastkaggle) (1.5.29)\nRequirement already satisfied: kaggle in /opt/conda/lib/python3.10/site-packages (from fastkaggle) (1.6.12)\nRequirement already satisfied: pip in /opt/conda/lib/python3.10/site-packages (from fastcore>=1.4.5->fastkaggle) (23.3.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from fastcore>=1.4.5->fastkaggle) (21.3)\nRequirement already satisfied: six>=1.10 in /opt/conda/lib/python3.10/site-packages (from kaggle->fastkaggle) (1.16.0)\nRequirement already satisfied: certifi>=2023.7.22 in /opt/conda/lib/python3.10/site-packages (from kaggle->fastkaggle) (2024.2.2)\nRequirement already satisfied: python-dateutil in /opt/conda/lib/python3.10/site-packages (from kaggle->fastkaggle) (2.9.0.post0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from kaggle->fastkaggle) (2.31.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from kaggle->fastkaggle) (4.66.1)\nRequirement already satisfied: python-slugify in /opt/conda/lib/python3.10/site-packages (from kaggle->fastkaggle) (8.0.4)\nRequirement already satisfied: urllib3 in /opt/conda/lib/python3.10/site-packages (from kaggle->fastkaggle) (1.26.18)\nRequirement already satisfied: bleach in /opt/conda/lib/python3.10/site-packages (from kaggle->fastkaggle) (6.1.0)\nRequirement already satisfied: webencodings in /opt/conda/lib/python3.10/site-packages (from bleach->kaggle->fastkaggle) (0.5.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->fastcore>=1.4.5->fastkaggle) (3.1.1)\nRequirement already satisfied: text-unidecode>=1.3 in /opt/conda/lib/python3.10/site-packages (from python-slugify->kaggle->fastkaggle) (1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->kaggle->fastkaggle) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->kaggle->fastkaggle) (3.6)\nDownloading fastkaggle-0.0.8-py3-none-any.whl (11 kB)\nInstalling collected packages: fastkaggle\nSuccessfully installed fastkaggle-0.0.8\nRequirement already satisfied: optuna in /opt/conda/lib/python3.10/site-packages (3.6.1)\nRequirement already satisfied: alembic>=1.5.0 in /opt/conda/lib/python3.10/site-packages (from optuna) (1.13.1)\nRequirement already satisfied: colorlog in /opt/conda/lib/python3.10/site-packages (from optuna) (6.8.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from optuna) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from optuna) (21.3)\nRequirement already satisfied: sqlalchemy>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from optuna) (2.0.25)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from optuna) (4.66.1)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from optuna) (6.0.1)\nRequirement already satisfied: Mako in /opt/conda/lib/python3.10/site-packages (from alembic>=1.5.0->optuna) (1.3.3)\nRequirement already satisfied: typing-extensions>=4 in /opt/conda/lib/python3.10/site-packages (from alembic>=1.5.0->optuna) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->optuna) (3.1.1)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from sqlalchemy>=1.3.0->optuna) (3.0.3)\nRequirement already satisfied: MarkupSafe>=0.9.2 in /opt/conda/lib/python3.10/site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.3)\nCollecting optuna_distributed\n  Downloading optuna_distributed-0.6.1-py3-none-any.whl.metadata (5.1 kB)\nRequirement already satisfied: optuna>=3.1.0 in /opt/conda/lib/python3.10/site-packages (from optuna_distributed) (3.6.1)\nRequirement already satisfied: dask[distributed] in /opt/conda/lib/python3.10/site-packages (from optuna_distributed) (2024.4.1)\nRequirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from optuna_distributed) (13.7.0)\nRequirement already satisfied: alembic>=1.5.0 in /opt/conda/lib/python3.10/site-packages (from optuna>=3.1.0->optuna_distributed) (1.13.1)\nRequirement already satisfied: colorlog in /opt/conda/lib/python3.10/site-packages (from optuna>=3.1.0->optuna_distributed) (6.8.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from optuna>=3.1.0->optuna_distributed) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from optuna>=3.1.0->optuna_distributed) (21.3)\nRequirement already satisfied: sqlalchemy>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from optuna>=3.1.0->optuna_distributed) (2.0.25)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from optuna>=3.1.0->optuna_distributed) (4.66.1)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from optuna>=3.1.0->optuna_distributed) (6.0.1)\nRequirement already satisfied: click>=8.1 in /opt/conda/lib/python3.10/site-packages (from dask[distributed]->optuna_distributed) (8.1.7)\nRequirement already satisfied: cloudpickle>=1.5.0 in /opt/conda/lib/python3.10/site-packages (from dask[distributed]->optuna_distributed) (2.2.1)\nRequirement already satisfied: fsspec>=2021.09.0 in /opt/conda/lib/python3.10/site-packages (from dask[distributed]->optuna_distributed) (2024.2.0)\nRequirement already satisfied: partd>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from dask[distributed]->optuna_distributed) (1.4.1)\nRequirement already satisfied: toolz>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from dask[distributed]->optuna_distributed) (0.12.1)\nRequirement already satisfied: importlib-metadata>=4.13.0 in /opt/conda/lib/python3.10/site-packages (from dask[distributed]->optuna_distributed) (6.11.0)\nCollecting distributed==2024.4.1 (from dask[distributed]->optuna_distributed)\n  Downloading distributed-2024.4.1-py3-none-any.whl.metadata (3.4 kB)\nRequirement already satisfied: jinja2>=2.10.3 in /opt/conda/lib/python3.10/site-packages (from distributed==2024.4.1->dask[distributed]->optuna_distributed) (3.1.2)\nRequirement already satisfied: locket>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from distributed==2024.4.1->dask[distributed]->optuna_distributed) (1.0.0)\nRequirement already satisfied: msgpack>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from distributed==2024.4.1->dask[distributed]->optuna_distributed) (1.0.7)\nRequirement already satisfied: psutil>=5.7.2 in /opt/conda/lib/python3.10/site-packages (from distributed==2024.4.1->dask[distributed]->optuna_distributed) (5.9.3)\nCollecting sortedcontainers>=2.0.5 (from distributed==2024.4.1->dask[distributed]->optuna_distributed)\n  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\nCollecting tblib>=1.6.0 (from distributed==2024.4.1->dask[distributed]->optuna_distributed)\n  Downloading tblib-3.0.0-py3-none-any.whl.metadata (25 kB)\nRequirement already satisfied: tornado>=6.0.4 in /opt/conda/lib/python3.10/site-packages (from distributed==2024.4.1->dask[distributed]->optuna_distributed) (6.3.3)\nRequirement already satisfied: urllib3>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from distributed==2024.4.1->dask[distributed]->optuna_distributed) (1.26.18)\nCollecting zict>=3.0.0 (from distributed==2024.4.1->dask[distributed]->optuna_distributed)\n  Downloading zict-3.0.0-py2.py3-none-any.whl.metadata (899 bytes)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->optuna_distributed) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->optuna_distributed) (2.17.2)\nRequirement already satisfied: Mako in /opt/conda/lib/python3.10/site-packages (from alembic>=1.5.0->optuna>=3.1.0->optuna_distributed) (1.3.3)\nRequirement already satisfied: typing-extensions>=4 in /opt/conda/lib/python3.10/site-packages (from alembic>=1.5.0->optuna>=3.1.0->optuna_distributed) (4.9.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata>=4.13.0->dask[distributed]->optuna_distributed) (3.17.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->optuna_distributed) (0.1.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->optuna>=3.1.0->optuna_distributed) (3.1.1)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from sqlalchemy>=1.3.0->optuna>=3.1.0->optuna_distributed) (3.0.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2>=2.10.3->distributed==2024.4.1->dask[distributed]->optuna_distributed) (2.1.3)\nDownloading optuna_distributed-0.6.1-py3-none-any.whl (30 kB)\nDownloading distributed-2024.4.1-py3-none-any.whl (1.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\nDownloading tblib-3.0.0-py3-none-any.whl (12 kB)\nDownloading zict-3.0.0-py2.py3-none-any.whl (43 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.3/43.3 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: sortedcontainers, zict, tblib, distributed, optuna_distributed\nSuccessfully installed distributed-2024.4.1 optuna_distributed-0.6.1 sortedcontainers-2.4.0 tblib-3.0.0 zict-3.0.0\n","output_type":"stream"}]},{"cell_type":"code","source":"#hide\n#! [ -e /content ]\n\n#hide\n#This imports and sets up everything you will need for this notebook\n#\n#!pip install -Uqq fastbook\n#import fastbook\n#fastbook.setup_book()\n\n#from fastbook import *\n#!pip install ucimlrepo\n#from ucimlrepo import fetch_ucirepo\n\nfrom fastai.tabular.all import *\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom numpy import random\n\nfrom fastai.imports import *\nnp.set_printoptions(linewidth=130)\n\n\nfrom pathlib import Path\nimport os\n\n\nfrom sklearn.ensemble import RandomForestRegressor,RandomForestClassifier\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error,r2_score\n#from sklearn.metrics import root_mean_squared_error\n\nimport xgboost as xgb\nfrom xgboost import plot_importance\n\nimport lightgbm as lgb\n\nfrom catboost import CatBoostClassifier,CatBoostRegressor,Pool, metrics, cv\n\nfrom ipywidgets import interact\n\n\nmatplotlib.rc('image', cmap='Greys')\n\nfrom fastkaggle import setup_comp\n\nimport optuna","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-05-08T16:11:15.94803Z","iopub.execute_input":"2024-05-08T16:11:15.948546Z","iopub.status.idle":"2024-05-08T16:11:18.274143Z","shell.execute_reply.started":"2024-05-08T16:11:15.948482Z","shell.execute_reply":"2024-05-08T16:11:18.272619Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"!ls /kaggle/input/playground-series-s4e5","metadata":{"execution":{"iopub.status.busy":"2024-05-08T16:11:18.276343Z","iopub.execute_input":"2024-05-08T16:11:18.277129Z","iopub.status.idle":"2024-05-08T16:11:19.412253Z","shell.execute_reply.started":"2024-05-08T16:11:18.27708Z","shell.execute_reply":"2024-05-08T16:11:19.410623Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"sample_submission.csv  test.csv  train.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"#set_seed(42)","metadata":{"execution":{"iopub.status.busy":"2024-05-08T06:13:55.053472Z","iopub.execute_input":"2024-05-08T06:13:55.053879Z","iopub.status.idle":"2024-05-08T06:13:55.059266Z","shell.execute_reply.started":"2024-05-08T06:13:55.053844Z","shell.execute_reply":"2024-05-08T06:13:55.05811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = Path('/kaggle/input/playground-series-s4e5/')\npath","metadata":{"execution":{"iopub.status.busy":"2024-05-08T16:11:43.661111Z","iopub.execute_input":"2024-05-08T16:11:43.661557Z","iopub.status.idle":"2024-05-08T16:11:43.671955Z","shell.execute_reply.started":"2024-05-08T16:11:43.661523Z","shell.execute_reply":"2024-05-08T16:11:43.670439Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"Path('/kaggle/input/playground-series-s4e5')"},"metadata":{}}]},{"cell_type":"markdown","source":"After some experimentation, i noticed adding the original dataset helps the model generalize better and produces better results.","metadata":{}},{"cell_type":"code","source":"!ls /kaggle/input/flood-prediction-factors","metadata":{"execution":{"iopub.status.busy":"2024-05-08T16:11:45.972805Z","iopub.execute_input":"2024-05-08T16:11:45.973359Z","iopub.status.idle":"2024-05-08T16:11:47.111007Z","shell.execute_reply.started":"2024-05-08T16:11:45.973318Z","shell.execute_reply":"2024-05-08T16:11:47.109464Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"flood.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"train_df = pd.read_csv(path/'train.csv',index_col='id')\ntest_df = pd.read_csv(path/'test.csv',index_col='id')\nsub_df = pd.read_csv(path/'sample_submission.csv',index_col='id')\noriginal_df = pd.read_csv('/kaggle/input/flood-prediction-factors/flood.csv')","metadata":{"execution":{"iopub.status.busy":"2024-05-08T16:11:50.111296Z","iopub.execute_input":"2024-05-08T16:11:50.111875Z","iopub.status.idle":"2024-05-08T16:11:55.354022Z","shell.execute_reply.started":"2024-05-08T16:11:50.111828Z","shell.execute_reply":"2024-05-08T16:11:55.352888Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"train_df.shape,original_df.shape","metadata":{"execution":{"iopub.status.busy":"2024-05-08T16:12:14.955513Z","iopub.execute_input":"2024-05-08T16:12:14.95631Z","iopub.status.idle":"2024-05-08T16:12:14.965666Z","shell.execute_reply.started":"2024-05-08T16:12:14.956256Z","shell.execute_reply":"2024-05-08T16:12:14.964287Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"((1167957, 21), (50000, 21))"},"metadata":{}}]},{"cell_type":"code","source":"train_df = pd.concat([train_df,original_df], axis=0)\ntrain_df.shape","metadata":{"execution":{"iopub.status.busy":"2024-05-08T16:11:55.356328Z","iopub.execute_input":"2024-05-08T16:11:55.356761Z","iopub.status.idle":"2024-05-08T16:11:55.441439Z","shell.execute_reply.started":"2024-05-08T16:11:55.356711Z","shell.execute_reply":"2024-05-08T16:11:55.44026Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"(1167957, 21)"},"metadata":{}}]},{"cell_type":"code","source":"cont_names,cat_names = cont_cat_split(train_df, dep_var='FloodProbability')","metadata":{"execution":{"iopub.status.busy":"2024-05-08T16:12:14.968247Z","iopub.execute_input":"2024-05-08T16:12:14.968899Z","iopub.status.idle":"2024-05-08T16:12:15.122536Z","shell.execute_reply.started":"2024-05-08T16:12:14.968862Z","shell.execute_reply":"2024-05-08T16:12:15.121278Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"splits = RandomSplitter(valid_pct=0.2)(range_of(train_df))","metadata":{"execution":{"iopub.status.busy":"2024-05-08T16:12:15.731728Z","iopub.execute_input":"2024-05-08T16:12:15.73228Z","iopub.status.idle":"2024-05-08T16:12:16.037085Z","shell.execute_reply.started":"2024-05-08T16:12:15.732237Z","shell.execute_reply":"2024-05-08T16:12:16.035969Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"to = TabularPandas(train_df, procs=[Categorify, FillMissing,Normalize],\n                   cat_names = cat_names,\n                   cont_names = cont_names,\n                   y_names='FloodProbability',\n                   y_block=RegressionBlock(),\n                   splits=splits)","metadata":{"execution":{"iopub.status.busy":"2024-05-08T16:12:16.402764Z","iopub.execute_input":"2024-05-08T16:12:16.403594Z","iopub.status.idle":"2024-05-08T16:12:17.404763Z","shell.execute_reply.started":"2024-05-08T16:12:16.403553Z","shell.execute_reply":"2024-05-08T16:12:17.403032Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"X_train, y_train = to.train.xs, to.train.ys.values.ravel()\nX_test, y_test = to.valid.xs, to.valid.ys.values.ravel()","metadata":{"execution":{"iopub.status.busy":"2024-05-08T16:12:18.921311Z","iopub.execute_input":"2024-05-08T16:12:18.922112Z","iopub.status.idle":"2024-05-08T16:12:19.01046Z","shell.execute_reply.started":"2024-05-08T16:12:18.922073Z","shell.execute_reply":"2024-05-08T16:12:19.009445Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"dls = to.dataloaders(bs=64)\ntest_dl = dls.test_dl(test_df)","metadata":{"execution":{"iopub.status.busy":"2024-05-08T16:12:19.39065Z","iopub.execute_input":"2024-05-08T16:12:19.391086Z","iopub.status.idle":"2024-05-08T16:12:19.707642Z","shell.execute_reply.started":"2024-05-08T16:12:19.391053Z","shell.execute_reply":"2024-05-08T16:12:19.706398Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"# Neural Network Baseline","metadata":{}},{"cell_type":"code","source":"learn = tabular_learner(dls, metrics=R2Score())\nlearn.lr_find(suggest_funcs=(slide,valley))","metadata":{"execution":{"iopub.status.busy":"2024-05-08T16:12:22.051502Z","iopub.execute_input":"2024-05-08T16:12:22.051976Z","iopub.status.idle":"2024-05-08T16:12:26.448227Z","shell.execute_reply.started":"2024-05-08T16:12:22.051934Z","shell.execute_reply":"2024-05-08T16:12:26.447009Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"SuggestedLRs(slide=0.009120108559727669, valley=0.0005754399462603033)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"%%time\nlearn.fit_one_cycle(10,0.02)","metadata":{"execution":{"iopub.status.busy":"2024-05-08T21:07:10.265402Z","iopub.execute_input":"2024-05-08T21:07:10.267847Z","iopub.status.idle":"2024-05-08T21:54:40.099138Z","shell.execute_reply.started":"2024-05-08T21:07:10.267783Z","shell.execute_reply":"2024-05-08T21:54:40.09772Z"},"trusted":true},"execution_count":61,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>epoch</th>\n      <th>train_loss</th>\n      <th>valid_loss</th>\n      <th>r2_score</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0.000485</td>\n      <td>0.000434</td>\n      <td>0.833338</td>\n      <td>04:53</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>0.000542</td>\n      <td>0.000418</td>\n      <td>0.839743</td>\n      <td>04:49</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.000517</td>\n      <td>0.000420</td>\n      <td>0.838975</td>\n      <td>04:43</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.000559</td>\n      <td>0.000437</td>\n      <td>0.832341</td>\n      <td>04:45</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.000523</td>\n      <td>0.000469</td>\n      <td>0.820106</td>\n      <td>04:41</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.000542</td>\n      <td>0.000497</td>\n      <td>0.809464</td>\n      <td>04:45</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.000487</td>\n      <td>0.000420</td>\n      <td>0.838998</td>\n      <td>04:45</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.000484</td>\n      <td>0.000415</td>\n      <td>0.840621</td>\n      <td>04:42</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.000424</td>\n      <td>0.000389</td>\n      <td>0.850636</td>\n      <td>04:42</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.000439</td>\n      <td>0.000386</td>\n      <td>0.851999</td>\n      <td>04:41</td>\n    </tr>\n  </tbody>\n</table>"},"metadata":{}},{"name":"stdout","text":"CPU times: user 1h 34min 12s, sys: 18.5 s, total: 1h 34min 31s\nWall time: 47min 29s\n","output_type":"stream"}]},{"cell_type":"code","source":"dl = learn.dls.test_dl(test_df)","metadata":{"execution":{"iopub.status.busy":"2024-05-08T21:55:15.748933Z","iopub.execute_input":"2024-05-08T21:55:15.749403Z","iopub.status.idle":"2024-05-08T21:55:16.415255Z","shell.execute_reply.started":"2024-05-08T21:55:15.749367Z","shell.execute_reply":"2024-05-08T21:55:16.413755Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"%%time\nnn_preds = learn.get_preds(dl=dl)\nnn_preds_x = learn.get_preds()[0]\na_preds, _ = learn.get_preds(dl=dl)\nnn_preds_y = a_preds.squeeze(1)","metadata":{"execution":{"iopub.status.busy":"2024-05-08T21:55:16.417899Z","iopub.execute_input":"2024-05-08T21:55:16.41836Z","iopub.status.idle":"2024-05-08T21:59:10.660172Z","shell.execute_reply.started":"2024-05-08T21:55:16.418326Z","shell.execute_reply":"2024-05-08T21:59:10.658719Z"},"trusted":true},"execution_count":63,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"CPU times: user 7min 44s, sys: 1.23 s, total: 7min 45s\nWall time: 3min 54s\n","output_type":"stream"}]},{"cell_type":"code","source":"!ls","metadata":{"execution":{"iopub.status.busy":"2024-05-08T21:59:10.662371Z","iopub.execute_input":"2024-05-08T21:59:10.66275Z","iopub.status.idle":"2024-05-08T21:59:11.873158Z","shell.execute_reply.started":"2024-05-08T21:59:10.662719Z","shell.execute_reply":"2024-05-08T21:59:11.871724Z"},"trusted":true},"execution_count":64,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"catboost_info  fp_model.pkl  models  submission.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"learn.export('models/fp_model.pkl')","metadata":{"execution":{"iopub.status.busy":"2024-05-08T17:01:48.854191Z","iopub.execute_input":"2024-05-08T17:01:48.854963Z","iopub.status.idle":"2024-05-08T17:01:51.066135Z","shell.execute_reply.started":"2024-05-08T17:01:48.854897Z","shell.execute_reply":"2024-05-08T17:01:51.064899Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"#learn.load('fp_model.pkl')","metadata":{"execution":{"iopub.status.busy":"2024-05-07T08:00:46.804312Z","iopub.execute_input":"2024-05-07T08:00:46.805275Z","iopub.status.idle":"2024-05-07T08:00:46.809381Z","shell.execute_reply.started":"2024-05-07T08:00:46.805239Z","shell.execute_reply":"2024-05-07T08:00:46.808408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"r2_score(y_test,nn_preds_x)","metadata":{"execution":{"iopub.status.busy":"2024-05-08T21:59:51.681066Z","iopub.execute_input":"2024-05-08T21:59:51.682605Z","iopub.status.idle":"2024-05-08T21:59:51.696375Z","shell.execute_reply.started":"2024-05-08T21:59:51.682551Z","shell.execute_reply":"2024-05-08T21:59:51.694957Z"},"trusted":true},"execution_count":65,"outputs":[{"execution_count":65,"output_type":"execute_result","data":{"text/plain":"0.8519993237262152"},"metadata":{}}]},{"cell_type":"code","source":"r2_score(y_test,nn_preds_x)","metadata":{"execution":{"iopub.status.busy":"2024-05-08T17:39:27.632183Z","iopub.execute_input":"2024-05-08T17:39:27.63264Z","iopub.status.idle":"2024-05-08T17:39:27.647082Z","shell.execute_reply.started":"2024-05-08T17:39:27.632604Z","shell.execute_reply":"2024-05-08T17:39:27.645613Z"},"trusted":true},"execution_count":41,"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"0.8550518897187577"},"metadata":{}}]},{"cell_type":"code","source":"target_preds = nn_preds[0]","metadata":{"execution":{"iopub.status.busy":"2024-05-08T17:02:56.968379Z","iopub.execute_input":"2024-05-08T17:02:56.96937Z","iopub.status.idle":"2024-05-08T17:02:56.975367Z","shell.execute_reply.started":"2024-05-08T17:02:56.969321Z","shell.execute_reply":"2024-05-08T17:02:56.97381Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"test_df['FloodProbability'] = target_preds","metadata":{"execution":{"iopub.status.busy":"2024-05-08T17:02:57.311045Z","iopub.execute_input":"2024-05-08T17:02:57.311456Z","iopub.status.idle":"2024-05-08T17:02:57.320644Z","shell.execute_reply.started":"2024-05-08T17:02:57.311424Z","shell.execute_reply":"2024-05-08T17:02:57.319509Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"test_df.to_csv('submission.csv', columns=['FloodProbability'], index=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-08T17:02:57.691171Z","iopub.execute_input":"2024-05-08T17:02:57.691597Z","iopub.status.idle":"2024-05-08T17:02:59.570696Z","shell.execute_reply.started":"2024-05-08T17:02:57.691564Z","shell.execute_reply":"2024-05-08T17:02:59.569397Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"submission = pd.read_csv('submission.csv')\nsubmission.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-08T17:02:59.996819Z","iopub.execute_input":"2024-05-08T17:02:59.997292Z","iopub.status.idle":"2024-05-08T17:03:00.274675Z","shell.execute_reply.started":"2024-05-08T17:02:59.997254Z","shell.execute_reply":"2024-05-08T17:03:00.273321Z"},"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"        id  FloodProbability\n0  1117957          0.573140\n1  1117958          0.453772\n2  1117959          0.453357\n3  1117960          0.464622\n4  1117961          0.465249","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>FloodProbability</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1117957</td>\n      <td>0.573140</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1117958</td>\n      <td>0.453772</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1117959</td>\n      <td>0.453357</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1117960</td>\n      <td>0.464622</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1117961</td>\n      <td>0.465249</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"!rm submission.csv","metadata":{"execution":{"iopub.status.busy":"2024-05-08T17:26:52.621178Z","iopub.execute_input":"2024-05-08T17:26:52.621659Z","iopub.status.idle":"2024-05-08T17:26:53.782852Z","shell.execute_reply.started":"2024-05-08T17:26:52.621618Z","shell.execute_reply":"2024-05-08T17:26:53.780712Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"test_df['FloodProbability'] = target_preds\ntest_df.to_csv('submission.csv', columns=['FloodProbability'], index=True)\n\nsubmission = pd.read_csv('submission.csv')\nsubmission.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-06T19:24:36.314231Z","iopub.execute_input":"2024-05-06T19:24:36.314513Z","iopub.status.idle":"2024-05-06T19:24:38.06162Z","shell.execute_reply.started":"2024-05-06T19:24:36.314488Z","shell.execute_reply":"2024-05-06T19:24:38.06067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Random Forest Baseline","metadata":{}},{"cell_type":"code","source":"rf = RandomForestRegressor(100, min_samples_leaf=3)\nrf_model = rf.fit(X_train, y_train);\n\nrf_preds = tensor(rf_model.predict(test_dl.xs))\n\nrf_preds_x = tensor(rf_model.predict(X_test))\n\nmse = mean_absolute_error(y_test, rf_preds_x)\nrmse = np.sqrt(mse)\n\nr2_score(y_test,rf_preds_x)","metadata":{"execution":{"iopub.status.busy":"2024-05-08T07:00:55.309688Z","iopub.execute_input":"2024-05-08T07:00:55.310216Z","iopub.status.idle":"2024-05-08T07:01:19.521748Z","shell.execute_reply.started":"2024-05-08T07:00:55.310185Z","shell.execute_reply":"2024-05-08T07:01:19.519994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls","metadata":{"execution":{"iopub.status.busy":"2024-05-07T10:41:45.976598Z","iopub.execute_input":"2024-05-07T10:41:45.977573Z","iopub.status.idle":"2024-05-07T10:41:47.04803Z","shell.execute_reply.started":"2024-05-07T10:41:45.977531Z","shell.execute_reply":"2024-05-07T10:41:47.046397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CatBoost Baseline","metadata":{}},{"cell_type":"code","source":"def objective_catboost(trial):\n    params = {\n        \"iterations\": 200,\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n        \"depth\": trial.suggest_int(\"depth\", 1, 10),\n        \"random_strength\": trial.suggest_int(\"random_strength\", 1, 10),\n    }\n    model = CatBoostRegressor(**params, silent=True)\n    model.fit(X_train, y_train, eval_set=(X_test, y_test), verbose=False)\n    cat_predictions = model.predict(X_test)\n    r2score = r2_score(y_test, cat_predictions)\n    return r2score\n\n# Create a study\nstudy_catboost = optuna.create_study(direction='minimize')\n\n# Convert the study to a distributed study\nstudy_catboost = optuna_distributed.from_study(study_catboost)\n\n# Run the optimization with parallel processing\nnum_parallel_jobs = 4 # Adjust this based on your system's capabilities\nstudy_catboost.optimize(objective_catboost, n_trials=40, n_jobs=num_parallel_jobs)\n\nprint(study_catboost.best_params)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import optuna\nfrom catboost import CatBoostRegressor\nfrom sklearn.metrics import r2_score\n\nimport optuna\nfrom catboost import CatBoostRegressor\nfrom sklearn.metrics import mean_squared_error\n\ndef objective_catboost(trial):\n    params = {\n        \"iterations\": 200,\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n        \"depth\": trial.suggest_int(\"depth\", 1, 10),\n        \"random_strength\": trial.suggest_int(\"random_strength\", 1, 10),\n    }\n    model = CatBoostRegressor(**params, silent=True)\n    model.fit(X_train, y_train, eval_set=(X_test, y_test), verbose=False)\n    predictions = model.predict(X_test)\n    rmse = mean_squared_error(y_test, predictions, squared=False)\n    return rmse\n\nstudy_catboost = optuna.create_study(direction='minimize')\nstudy_catboost.optimize(objective_catboost, n_trials=100)\nprint(study_catboost.best_params)","metadata":{"execution":{"iopub.status.busy":"2024-05-08T17:42:04.953174Z","iopub.execute_input":"2024-05-08T17:42:04.953626Z","iopub.status.idle":"2024-05-08T18:36:26.896438Z","shell.execute_reply.started":"2024-05-08T17:42:04.953588Z","shell.execute_reply":"2024-05-08T18:36:26.895217Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stderr","text":"[I 2024-05-08 17:42:04,960] A new study created in memory with name: no-name-4676c310-1db9-4fc2-827e-ebc389fb1566\n[I 2024-05-08 17:42:34,702] Trial 0 finished with value: 0.03215043510245607 and parameters: {'learning_rate': 0.01768895341952706, 'depth': 8, 'random_strength': 8}. Best is trial 0 with value: 0.03215043510245607.\n[I 2024-05-08 17:42:50,245] Trial 1 finished with value: 0.022578923672200837 and parameters: {'learning_rate': 0.1325284168693795, 'depth': 3, 'random_strength': 7}. Best is trial 1 with value: 0.022578923672200837.\n[I 2024-05-08 17:43:19,238] Trial 2 finished with value: 0.029759591906668062 and parameters: {'learning_rate': 0.022399694152837702, 'depth': 8, 'random_strength': 9}. Best is trial 1 with value: 0.022578923672200837.\n[I 2024-05-08 17:43:40,002] Trial 3 finished with value: 0.020364262822405454 and parameters: {'learning_rate': 0.11852094380370713, 'depth': 6, 'random_strength': 5}. Best is trial 3 with value: 0.020364262822405454.\n[I 2024-05-08 17:44:03,467] Trial 4 finished with value: 0.0315178086375991 and parameters: {'learning_rate': 0.02509987295551062, 'depth': 6, 'random_strength': 1}. Best is trial 3 with value: 0.020364262822405454.\n[I 2024-05-08 17:44:25,684] Trial 5 finished with value: 0.03422254310079697 and parameters: {'learning_rate': 0.023198877257304885, 'depth': 5, 'random_strength': 6}. Best is trial 3 with value: 0.020364262822405454.\n[I 2024-05-08 17:44:46,167] Trial 6 finished with value: 0.019999650976907722 and parameters: {'learning_rate': 0.15224435109605725, 'depth': 6, 'random_strength': 9}. Best is trial 6 with value: 0.019999650976907722.\n[I 2024-05-08 17:45:01,231] Trial 7 finished with value: 0.020561947685010223 and parameters: {'learning_rate': 0.20671331107113755, 'depth': 3, 'random_strength': 7}. Best is trial 6 with value: 0.019999650976907722.\n[I 2024-05-08 17:45:32,310] Trial 8 finished with value: 0.036369492767585526 and parameters: {'learning_rate': 0.011586934617463258, 'depth': 8, 'random_strength': 10}. Best is trial 6 with value: 0.019999650976907722.\n[I 2024-05-08 17:46:36,982] Trial 9 finished with value: 0.019831372996702002 and parameters: {'learning_rate': 0.15470210991140218, 'depth': 10, 'random_strength': 1}. Best is trial 9 with value: 0.019831372996702002.\n[I 2024-05-08 17:47:41,469] Trial 10 finished with value: 0.020631425525377597 and parameters: {'learning_rate': 0.0655686208141312, 'depth': 10, 'random_strength': 1}. Best is trial 9 with value: 0.019831372996702002.\n[I 2024-05-08 17:47:53,588] Trial 11 finished with value: 0.024535148188926516 and parameters: {'learning_rate': 0.2809766026618507, 'depth': 1, 'random_strength': 4}. Best is trial 9 with value: 0.019831372996702002.\n[I 2024-05-08 17:48:57,731] Trial 12 finished with value: 0.020561534989591585 and parameters: {'learning_rate': 0.06799439314027286, 'depth': 10, 'random_strength': 3}. Best is trial 9 with value: 0.019831372996702002.\n[I 2024-05-08 17:49:15,141] Trial 13 finished with value: 0.021690155935984275 and parameters: {'learning_rate': 0.11780466704214633, 'depth': 4, 'random_strength': 3}. Best is trial 9 with value: 0.019831372996702002.\n[I 2024-05-08 17:49:40,402] Trial 14 finished with value: 0.025367003715031495 and parameters: {'learning_rate': 0.040569860505058, 'depth': 7, 'random_strength': 10}. Best is trial 9 with value: 0.019831372996702002.\n[I 2024-05-08 17:50:05,732] Trial 15 finished with value: 0.019816722843022976 and parameters: {'learning_rate': 0.18473333772596448, 'depth': 9, 'random_strength': 2}. Best is trial 15 with value: 0.019816722843022976.\n[I 2024-05-08 17:51:10,580] Trial 16 finished with value: 0.019793105684556722 and parameters: {'learning_rate': 0.25084407192607344, 'depth': 10, 'random_strength': 2}. Best is trial 16 with value: 0.019793105684556722.\n[I 2024-05-08 17:51:36,766] Trial 17 finished with value: 0.019781373287561983 and parameters: {'learning_rate': 0.29840807972844996, 'depth': 9, 'random_strength': 3}. Best is trial 17 with value: 0.019781373287561983.\n[I 2024-05-08 17:52:02,198] Trial 18 finished with value: 0.019786432103230257 and parameters: {'learning_rate': 0.2880962514827134, 'depth': 9, 'random_strength': 4}. Best is trial 17 with value: 0.019781373287561983.\n[I 2024-05-08 17:52:28,668] Trial 19 finished with value: 0.020011924932361758 and parameters: {'learning_rate': 0.10473615708545758, 'depth': 9, 'random_strength': 5}. Best is trial 17 with value: 0.019781373287561983.\n[I 2024-05-08 17:52:52,270] Trial 20 finished with value: 0.021005604733143477 and parameters: {'learning_rate': 0.0810825718482521, 'depth': 7, 'random_strength': 4}. Best is trial 17 with value: 0.019781373287561983.\n[I 2024-05-08 17:53:18,143] Trial 21 finished with value: 0.01978362878275635 and parameters: {'learning_rate': 0.2917233814981826, 'depth': 9, 'random_strength': 3}. Best is trial 17 with value: 0.019781373287561983.\n[I 2024-05-08 17:53:43,638] Trial 22 finished with value: 0.019787907483209532 and parameters: {'learning_rate': 0.29487636211131385, 'depth': 9, 'random_strength': 3}. Best is trial 17 with value: 0.019781373287561983.\n[I 2024-05-08 17:54:04,800] Trial 23 finished with value: 0.01980772492363192 and parameters: {'learning_rate': 0.21113816127165883, 'depth': 7, 'random_strength': 4}. Best is trial 17 with value: 0.019781373287561983.\n[I 2024-05-08 17:54:33,744] Trial 24 finished with value: 0.022961491581933467 and parameters: {'learning_rate': 0.042825334519741284, 'depth': 9, 'random_strength': 2}. Best is trial 17 with value: 0.019781373287561983.\n[I 2024-05-08 17:54:57,092] Trial 25 finished with value: 0.019802999071363316 and parameters: {'learning_rate': 0.22121419260991854, 'depth': 8, 'random_strength': 4}. Best is trial 17 with value: 0.019781373287561983.\n[I 2024-05-08 17:55:23,204] Trial 26 finished with value: 0.01979225810996671 and parameters: {'learning_rate': 0.29342611772559113, 'depth': 9, 'random_strength': 6}. Best is trial 17 with value: 0.019781373287561983.\n[I 2024-05-08 17:55:44,868] Trial 27 finished with value: 0.01984018376827853 and parameters: {'learning_rate': 0.17287430175402188, 'depth': 7, 'random_strength': 3}. Best is trial 17 with value: 0.019781373287561983.\n[I 2024-05-08 17:56:09,817] Trial 28 finished with value: 0.020280971994148148 and parameters: {'learning_rate': 0.09449900616010774, 'depth': 8, 'random_strength': 5}. Best is trial 17 with value: 0.019781373287561983.\n[I 2024-05-08 17:56:33,102] Trial 29 finished with value: 0.01979535706588626 and parameters: {'learning_rate': 0.23663816576445768, 'depth': 8, 'random_strength': 2}. Best is trial 17 with value: 0.019781373287561983.\n[I 2024-05-08 17:56:51,433] Trial 30 finished with value: 0.020074276726066008 and parameters: {'learning_rate': 0.16670296434484597, 'depth': 5, 'random_strength': 3}. Best is trial 17 with value: 0.019781373287561983.\n[I 2024-05-08 17:57:17,468] Trial 31 finished with value: 0.019780121548498418 and parameters: {'learning_rate': 0.29527167043257596, 'depth': 9, 'random_strength': 3}. Best is trial 31 with value: 0.019780121548498418.\n[I 2024-05-08 17:57:43,011] Trial 32 finished with value: 0.01980493070233597 and parameters: {'learning_rate': 0.22937210316025952, 'depth': 9, 'random_strength': 4}. Best is trial 31 with value: 0.019780121548498418.\n[I 2024-05-08 17:58:48,492] Trial 33 finished with value: 0.019846438830798584 and parameters: {'learning_rate': 0.13938005305074783, 'depth': 10, 'random_strength': 2}. Best is trial 31 with value: 0.019780121548498418.\n[I 2024-05-08 17:59:11,770] Trial 34 finished with value: 0.01979728235767617 and parameters: {'learning_rate': 0.2870515116442014, 'depth': 8, 'random_strength': 3}. Best is trial 31 with value: 0.019780121548498418.\n[I 2024-05-08 17:59:37,057] Trial 35 finished with value: 0.019822392717618494 and parameters: {'learning_rate': 0.18644970808778236, 'depth': 9, 'random_strength': 5}. Best is trial 31 with value: 0.019780121548498418.\n[I 2024-05-08 18:00:41,836] Trial 36 finished with value: 0.01979489471977934 and parameters: {'learning_rate': 0.2431601964857728, 'depth': 10, 'random_strength': 4}. Best is trial 31 with value: 0.019780121548498418.\n[I 2024-05-08 18:01:04,272] Trial 37 finished with value: 0.020041287997678757 and parameters: {'learning_rate': 0.12598044278217835, 'depth': 7, 'random_strength': 6}. Best is trial 31 with value: 0.019780121548498418.\n[I 2024-05-08 18:01:28,659] Trial 38 finished with value: 0.029830814918144405 and parameters: {'learning_rate': 0.029640127178820887, 'depth': 6, 'random_strength': 5}. Best is trial 31 with value: 0.019780121548498418.\n[I 2024-05-08 18:01:40,293] Trial 39 finished with value: 0.04756745140244556 and parameters: {'learning_rate': 0.015798084603314777, 'depth': 1, 'random_strength': 7}. Best is trial 31 with value: 0.019780121548498418.\n[I 2024-05-08 18:02:03,854] Trial 40 finished with value: 0.019803020348039452 and parameters: {'learning_rate': 0.2029212121400651, 'depth': 8, 'random_strength': 1}. Best is trial 31 with value: 0.019780121548498418.\n[I 2024-05-08 18:02:29,770] Trial 41 finished with value: 0.019786027863178617 and parameters: {'learning_rate': 0.28452673133398537, 'depth': 9, 'random_strength': 3}. Best is trial 31 with value: 0.019780121548498418.\n[I 2024-05-08 18:02:55,273] Trial 42 finished with value: 0.019794997045253702 and parameters: {'learning_rate': 0.2649638574459708, 'depth': 9, 'random_strength': 3}. Best is trial 31 with value: 0.019780121548498418.\n[I 2024-05-08 18:03:59,775] Trial 43 finished with value: 0.019790071799227992 and parameters: {'learning_rate': 0.2454058153263613, 'depth': 10, 'random_strength': 2}. Best is trial 31 with value: 0.019780121548498418.\n[I 2024-05-08 18:04:22,860] Trial 44 finished with value: 0.019856697560099 and parameters: {'learning_rate': 0.15070705555192962, 'depth': 8, 'random_strength': 4}. Best is trial 31 with value: 0.019780121548498418.\n[I 2024-05-08 18:04:36,714] Trial 45 finished with value: 0.020538374128059228 and parameters: {'learning_rate': 0.2988383836865176, 'depth': 2, 'random_strength': 3}. Best is trial 31 with value: 0.019780121548498418.\n[I 2024-05-08 18:05:01,633] Trial 46 finished with value: 0.019806108070439764 and parameters: {'learning_rate': 0.1970708695763305, 'depth': 9, 'random_strength': 1}. Best is trial 31 with value: 0.019780121548498418.\n[I 2024-05-08 18:06:06,016] Trial 47 finished with value: 0.019792995447358876 and parameters: {'learning_rate': 0.2464371246617666, 'depth': 10, 'random_strength': 2}. Best is trial 31 with value: 0.019780121548498418.\n[I 2024-05-08 18:06:24,301] Trial 48 finished with value: 0.020041988259746693 and parameters: {'learning_rate': 0.1714483391512047, 'depth': 5, 'random_strength': 3}. Best is trial 31 with value: 0.019780121548498418.\n[I 2024-05-08 18:06:47,323] Trial 49 finished with value: 0.019803502380982836 and parameters: {'learning_rate': 0.20924054575884093, 'depth': 8, 'random_strength': 4}. Best is trial 31 with value: 0.019780121548498418.\n[I 2024-05-08 18:07:04,399] Trial 50 finished with value: 0.020991459454831624 and parameters: {'learning_rate': 0.1385562494409824, 'depth': 4, 'random_strength': 5}. Best is trial 31 with value: 0.019780121548498418.\n[I 2024-05-08 18:07:30,409] Trial 51 finished with value: 0.01977612782018991 and parameters: {'learning_rate': 0.29556991056162324, 'depth': 9, 'random_strength': 3}. Best is trial 51 with value: 0.01977612782018991.\n[I 2024-05-08 18:07:55,836] Trial 52 finished with value: 0.019795227365215103 and parameters: {'learning_rate': 0.26351339951890856, 'depth': 9, 'random_strength': 3}. Best is trial 51 with value: 0.01977612782018991.\n[I 2024-05-08 18:09:00,142] Trial 53 finished with value: 0.01979308674872381 and parameters: {'learning_rate': 0.2632674888201994, 'depth': 10, 'random_strength': 2}. Best is trial 51 with value: 0.01977612782018991.\n[I 2024-05-08 18:09:25,506] Trial 54 finished with value: 0.019800633935934887 and parameters: {'learning_rate': 0.22244635967778734, 'depth': 9, 'random_strength': 4}. Best is trial 51 with value: 0.01977612782018991.\n[I 2024-05-08 18:09:51,275] Trial 55 finished with value: 0.019784945252269873 and parameters: {'learning_rate': 0.29934764807937214, 'depth': 9, 'random_strength': 3}. Best is trial 51 with value: 0.01977612782018991.\n[I 2024-05-08 18:10:55,695] Trial 56 finished with value: 0.019782579874701167 and parameters: {'learning_rate': 0.25425989577151886, 'depth': 10, 'random_strength': 3}. Best is trial 51 with value: 0.01977612782018991.\n[I 2024-05-08 18:11:59,983] Trial 57 finished with value: 0.019825672727896624 and parameters: {'learning_rate': 0.18908286427568105, 'depth': 10, 'random_strength': 8}. Best is trial 51 with value: 0.01977612782018991.\n[I 2024-05-08 18:13:04,300] Trial 58 finished with value: 0.01980050745629769 and parameters: {'learning_rate': 0.2231659302280735, 'depth': 10, 'random_strength': 2}. Best is trial 51 with value: 0.01977612782018991.\n[I 2024-05-08 18:14:08,414] Trial 59 finished with value: 0.03430914589195214 and parameters: {'learning_rate': 0.011299728975546215, 'depth': 10, 'random_strength': 1}. Best is trial 51 with value: 0.01977612782018991.\n[I 2024-05-08 18:14:31,793] Trial 60 finished with value: 0.019838856161781056 and parameters: {'learning_rate': 0.15971881188385018, 'depth': 8, 'random_strength': 2}. Best is trial 51 with value: 0.01977612782018991.\n[I 2024-05-08 18:14:57,927] Trial 61 finished with value: 0.019771158869633057 and parameters: {'learning_rate': 0.29776650862748444, 'depth': 9, 'random_strength': 3}. Best is trial 61 with value: 0.019771158869633057.\n[I 2024-05-08 18:15:24,032] Trial 62 finished with value: 0.019790425284730396 and parameters: {'learning_rate': 0.29961888929493286, 'depth': 9, 'random_strength': 3}. Best is trial 61 with value: 0.019771158869633057.\n[I 2024-05-08 18:15:49,591] Trial 63 finished with value: 0.019796397613202225 and parameters: {'learning_rate': 0.25563653868008623, 'depth': 9, 'random_strength': 3}. Best is trial 61 with value: 0.019771158869633057.\n[I 2024-05-08 18:16:13,071] Trial 64 finished with value: 0.019794337521492983 and parameters: {'learning_rate': 0.26384259122499715, 'depth': 8, 'random_strength': 4}. Best is trial 61 with value: 0.019771158869633057.\n[I 2024-05-08 18:17:17,803] Trial 65 finished with value: 0.021791427144204057 and parameters: {'learning_rate': 0.047856753054392895, 'depth': 10, 'random_strength': 3}. Best is trial 61 with value: 0.019771158869633057.\n[I 2024-05-08 18:17:42,705] Trial 66 finished with value: 0.019801600275614024 and parameters: {'learning_rate': 0.22029676282768898, 'depth': 9, 'random_strength': 2}. Best is trial 61 with value: 0.019771158869633057.\n[I 2024-05-08 18:18:04,335] Trial 67 finished with value: 0.019820614139585604 and parameters: {'learning_rate': 0.19270134162409003, 'depth': 7, 'random_strength': 4}. Best is trial 61 with value: 0.019771158869633057.\n[I 2024-05-08 18:19:08,720] Trial 68 finished with value: 0.020649247567553385 and parameters: {'learning_rate': 0.06557591791881634, 'depth': 10, 'random_strength': 3}. Best is trial 61 with value: 0.019771158869633057.\n[I 2024-05-08 18:19:34,216] Trial 69 finished with value: 0.01978832017222587 and parameters: {'learning_rate': 0.24107487573562986, 'depth': 9, 'random_strength': 2}. Best is trial 61 with value: 0.019771158869633057.\n[I 2024-05-08 18:19:57,706] Trial 70 finished with value: 0.019793031232322816 and parameters: {'learning_rate': 0.2702758456986307, 'depth': 8, 'random_strength': 4}. Best is trial 61 with value: 0.019771158869633057.\n[I 2024-05-08 18:20:23,199] Trial 71 finished with value: 0.019787127644288024 and parameters: {'learning_rate': 0.2747805092490611, 'depth': 9, 'random_strength': 3}. Best is trial 61 with value: 0.019771158869633057.\n[I 2024-05-08 18:20:49,179] Trial 72 finished with value: 0.019779086796714713 and parameters: {'learning_rate': 0.2979204938017501, 'depth': 9, 'random_strength': 3}. Best is trial 61 with value: 0.019771158869633057.\n[I 2024-05-08 18:21:12,422] Trial 73 finished with value: 0.019790351003061254 and parameters: {'learning_rate': 0.2995925693247667, 'depth': 8, 'random_strength': 3}. Best is trial 61 with value: 0.019771158869633057.\n[I 2024-05-08 18:21:37,925] Trial 74 finished with value: 0.019795611328945903 and parameters: {'learning_rate': 0.2325636534544115, 'depth': 9, 'random_strength': 2}. Best is trial 61 with value: 0.019771158869633057.\n[I 2024-05-08 18:22:42,422] Trial 75 finished with value: 0.019807697796176683 and parameters: {'learning_rate': 0.21069645797238862, 'depth': 10, 'random_strength': 3}. Best is trial 61 with value: 0.019771158869633057.\n[I 2024-05-08 18:23:07,528] Trial 76 finished with value: 0.019813038476425227 and parameters: {'learning_rate': 0.17999441700715127, 'depth': 9, 'random_strength': 4}. Best is trial 61 with value: 0.019771158869633057.\n[I 2024-05-08 18:24:12,650] Trial 77 finished with value: 0.024694168853215608 and parameters: {'learning_rate': 0.030451919097196377, 'depth': 10, 'random_strength': 3}. Best is trial 61 with value: 0.019771158869633057.\n[I 2024-05-08 18:24:35,709] Trial 78 finished with value: 0.019792990462535336 and parameters: {'learning_rate': 0.24842959959222427, 'depth': 8, 'random_strength': 2}. Best is trial 61 with value: 0.019771158869633057.\n[I 2024-05-08 18:25:02,017] Trial 79 finished with value: 0.019793718074707832 and parameters: {'learning_rate': 0.27205414962115476, 'depth': 9, 'random_strength': 5}. Best is trial 61 with value: 0.019771158869633057.\n[I 2024-05-08 18:26:06,800] Trial 80 finished with value: 0.03253650001053659 and parameters: {'learning_rate': 0.01352696731779323, 'depth': 10, 'random_strength': 3}. Best is trial 61 with value: 0.019771158869633057.\n[I 2024-05-08 18:26:32,578] Trial 81 finished with value: 0.019789935625157767 and parameters: {'learning_rate': 0.279192827618579, 'depth': 9, 'random_strength': 3}. Best is trial 61 with value: 0.019771158869633057.\n[I 2024-05-08 18:26:57,938] Trial 82 finished with value: 0.019804627415749922 and parameters: {'learning_rate': 0.23516854829303915, 'depth': 9, 'random_strength': 4}. Best is trial 61 with value: 0.019771158869633057.\n[I 2024-05-08 18:27:23,602] Trial 83 finished with value: 0.019782387055341674 and parameters: {'learning_rate': 0.29865730520677247, 'depth': 9, 'random_strength': 3}. Best is trial 61 with value: 0.019771158869633057.\n[I 2024-05-08 18:27:48,959] Trial 84 finished with value: 0.01981214779867872 and parameters: {'learning_rate': 0.20393896052597035, 'depth': 9, 'random_strength': 3}. Best is trial 61 with value: 0.019771158869633057.\n[I 2024-05-08 18:28:12,288] Trial 85 finished with value: 0.019794751321852686 and parameters: {'learning_rate': 0.2983323332917301, 'depth': 8, 'random_strength': 4}. Best is trial 61 with value: 0.019771158869633057.\n[I 2024-05-08 18:28:33,574] Trial 86 finished with value: 0.019796491912450767 and parameters: {'learning_rate': 0.2512041159499292, 'depth': 7, 'random_strength': 3}. Best is trial 61 with value: 0.019771158869633057.\n[I 2024-05-08 18:28:58,668] Trial 87 finished with value: 0.01978370693298287 and parameters: {'learning_rate': 0.2740971892698603, 'depth': 9, 'random_strength': 2}. Best is trial 61 with value: 0.019771158869633057.\n[I 2024-05-08 18:30:03,026] Trial 88 finished with value: 0.01979237304224087 and parameters: {'learning_rate': 0.22998409616194979, 'depth': 10, 'random_strength': 1}. Best is trial 61 with value: 0.019771158869633057.\n[I 2024-05-08 18:30:32,244] Trial 89 finished with value: 0.030442629692926396 and parameters: {'learning_rate': 0.02080306813619149, 'depth': 8, 'random_strength': 2}. Best is trial 61 with value: 0.019771158869633057.\n[I 2024-05-08 18:30:51,880] Trial 90 finished with value: 0.01979302105819975 and parameters: {'learning_rate': 0.27053953636989125, 'depth': 6, 'random_strength': 2}. Best is trial 61 with value: 0.019771158869633057.\n[I 2024-05-08 18:31:17,805] Trial 91 finished with value: 0.019785393186045115 and parameters: {'learning_rate': 0.2845912520873473, 'depth': 9, 'random_strength': 3}. Best is trial 61 with value: 0.019771158869633057.\n[I 2024-05-08 18:31:43,154] Trial 92 finished with value: 0.01978640265524411 and parameters: {'learning_rate': 0.25288942652942037, 'depth': 9, 'random_strength': 2}. Best is trial 61 with value: 0.019771158869633057.\n[I 2024-05-08 18:32:08,483] Trial 93 finished with value: 0.01979095600108117 and parameters: {'learning_rate': 0.29890016728075597, 'depth': 9, 'random_strength': 3}. Best is trial 61 with value: 0.019771158869633057.\n[I 2024-05-08 18:32:33,681] Trial 94 finished with value: 0.019807143448113195 and parameters: {'learning_rate': 0.2134540058413679, 'depth': 9, 'random_strength': 4}. Best is trial 61 with value: 0.019771158869633057.\n[I 2024-05-08 18:33:37,912] Trial 95 finished with value: 0.02021053889779275 and parameters: {'learning_rate': 0.08143368361118455, 'depth': 10, 'random_strength': 3}. Best is trial 61 with value: 0.019771158869633057.\n[I 2024-05-08 18:33:54,717] Trial 96 finished with value: 0.01984699993255575 and parameters: {'learning_rate': 0.27313271355052615, 'depth': 4, 'random_strength': 2}. Best is trial 61 with value: 0.019771158869633057.\n[I 2024-05-08 18:34:17,419] Trial 97 finished with value: 0.01979638632314656 and parameters: {'learning_rate': 0.23688963942909216, 'depth': 8, 'random_strength': 3}. Best is trial 61 with value: 0.019771158869633057.\n[I 2024-05-08 18:35:22,059] Trial 98 finished with value: 0.019782638579549643 and parameters: {'learning_rate': 0.256477481465989, 'depth': 10, 'random_strength': 1}. Best is trial 61 with value: 0.019771158869633057.\n[I 2024-05-08 18:36:26,890] Trial 99 finished with value: 0.019804214121410136 and parameters: {'learning_rate': 0.19954922748859774, 'depth': 10, 'random_strength': 1}. Best is trial 61 with value: 0.019771158869633057.\n","output_type":"stream"},{"name":"stdout","text":"{'learning_rate': 0.29776650862748444, 'depth': 9, 'random_strength': 3}\n","output_type":"stream"}]},{"cell_type":"code","source":"#with optuna params\ncat_model = CatBoostRegressor(iterations=1441, depth=9, learning_rate= 0.29776650862748444, random_strength=3)\ncat_model = cat_model.fit(X_train, y_train, eval_set=(X_test, y_test), verbose=False)\n\n#test set preds\ncat_preds = tensor(cat_model.predict(test_dl.xs))\n\n#validation set preds\ncat_preds_x = tensor(cat_model.predict(X_test))\n\nr2_score(y_test,cat_preds_x)","metadata":{"execution":{"iopub.status.busy":"2024-05-08T20:19:44.289854Z","iopub.execute_input":"2024-05-08T20:19:44.290336Z","iopub.status.idle":"2024-05-08T20:22:57.241717Z","shell.execute_reply.started":"2024-05-08T20:19:44.290288Z","shell.execute_reply":"2024-05-08T20:22:57.240379Z"},"trusted":true},"execution_count":46,"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"0.8508502177305014"},"metadata":{}}]},{"cell_type":"code","source":"#without optuna params\ncat_model = CatBoostRegressor(iterations=500, depth=10, learning_rate= 0.08, random_strength=8)\ncat_model = cat_model.fit(X_train, y_train, eval_set=(X_test, y_test), verbose=False)\n\n#test set preds\ncat_preds = tensor(cat_model.predict(test_dl.xs))\n\n#validation set preds\ncat_preds_x = tensor(cat_model.predict(X_test))\n\nr2_score(y_test,cat_preds_x)","metadata":{"execution":{"iopub.status.busy":"2024-05-08T17:26:59.702707Z","iopub.execute_input":"2024-05-08T17:26:59.703794Z","iopub.status.idle":"2024-05-08T17:30:39.311976Z","shell.execute_reply.started":"2024-05-08T17:26:59.703747Z","shell.execute_reply":"2024-05-08T17:30:39.310631Z"},"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"0.8497849381599005"},"metadata":{}}]},{"cell_type":"markdown","source":"# Light GBM","metadata":{}},{"cell_type":"code","source":"def objective_lgbm(trial):\n    params = {\n        \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 50),\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 200),\n    }\n    model = lgb.LGBMRegressor(**params)\n    model.fit(X_train, y_train)\n    lgb_predictions = model.predict(X_test)\n    r2score = r2_score(y_test, lgb_predictions)\n    return r2score\n\n# Create a study\nstudy_lgbm = optuna.create_study(direction='minimize')\n\n# Convert the study to a distributed study\nstudy_lgbm = optuna_distributed.from_study(study_lgbm)\n\n# Run the optimization with parallel processing\nnum_parallel_jobs = 4 # Adjust this based on your system's capabilities\nstudy_lgbm.optimize(objective_lgbm, n_trials=100, n_jobs=num_parallel_jobs)\n\nprint(study_lgbm.best_params)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def objective_lgbm(trial):\n    params = {\n        \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 50),\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 200),\n    }\n    model = lgb.LGBMRegressor(**params)\n    model.fit(X_train, y_train)\n    lgb_predictions = model.predict(X_test)\n    rmse = mean_squared_error(y_test, lgb_predictions, squared=False)\n    return rmse\n\nstudy_lgbm = optuna.create_study(direction='minimize')\nstudy_lgbm.optimize(objective_lgbm, n_trials=100)\nprint(study_lgbm.best_params)","metadata":{"execution":{"iopub.status.busy":"2024-05-08T18:45:29.424103Z","iopub.execute_input":"2024-05-08T18:45:29.424635Z","iopub.status.idle":"2024-05-08T19:37:02.817357Z","shell.execute_reply.started":"2024-05-08T18:45:29.424592Z","shell.execute_reply":"2024-05-08T19:37:02.815422Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stderr","text":"[I 2024-05-08 18:45:29,431] A new study created in memory with name: no-name-bbf13ecf-6932-4899-95f7-c14a8055a3e2\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.272966 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 18:46:07,205] Trial 0 finished with value: 0.022216366835694275 and parameters: {'num_leaves': 35, 'learning_rate': 0.0636400244864595, 'n_estimators': 200}. Best is trial 0 with value: 0.022216366835694275.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.263284 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 18:46:36,095] Trial 1 finished with value: 0.03530875442830606 and parameters: {'num_leaves': 24, 'learning_rate': 0.025974734066045564, 'n_estimators': 156}. Best is trial 0 with value: 0.022216366835694275.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.188151 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 18:46:56,959] Trial 2 finished with value: 0.021233515295115378 and parameters: {'num_leaves': 49, 'learning_rate': 0.14416347886142988, 'n_estimators': 115}. Best is trial 2 with value: 0.021233515295115378.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.182860 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 18:47:28,334] Trial 3 finished with value: 0.021222144137371418 and parameters: {'num_leaves': 49, 'learning_rate': 0.08515872386244602, 'n_estimators': 174}. Best is trial 3 with value: 0.021222144137371418.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.184202 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 18:47:50,026] Trial 4 finished with value: 0.02140309420963228 and parameters: {'num_leaves': 49, 'learning_rate': 0.12733886407274667, 'n_estimators': 117}. Best is trial 3 with value: 0.021222144137371418.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.186227 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 18:48:09,580] Trial 5 finished with value: 0.02806667623088059 and parameters: {'num_leaves': 50, 'learning_rate': 0.06077212755768868, 'n_estimators': 100}. Best is trial 3 with value: 0.021222144137371418.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.182196 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 18:48:34,069] Trial 6 finished with value: 0.02123631447194069 and parameters: {'num_leaves': 47, 'learning_rate': 0.20699764591483164, 'n_estimators': 175}. Best is trial 3 with value: 0.021222144137371418.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.189893 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 18:49:12,025] Trial 7 finished with value: 0.028181466701508055 and parameters: {'num_leaves': 38, 'learning_rate': 0.033212866398653076, 'n_estimators': 196}. Best is trial 3 with value: 0.021222144137371418.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.263055 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 18:49:43,625] Trial 8 finished with value: 0.020995284799478892 and parameters: {'num_leaves': 44, 'learning_rate': 0.17859213272862978, 'n_estimators': 180}. Best is trial 8 with value: 0.020995284799478892.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.244548 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 18:50:15,877] Trial 9 finished with value: 0.04232938644978339 and parameters: {'num_leaves': 41, 'learning_rate': 0.011070266303811239, 'n_estimators': 144}. Best is trial 8 with value: 0.020995284799478892.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.259534 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 18:50:38,724] Trial 10 finished with value: 0.021468477990886782 and parameters: {'num_leaves': 29, 'learning_rate': 0.28403266242997427, 'n_estimators': 175}. Best is trial 8 with value: 0.020995284799478892.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.260364 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 18:51:14,023] Trial 11 finished with value: 0.02089347666583302 and parameters: {'num_leaves': 42, 'learning_rate': 0.10285423909419207, 'n_estimators': 176}. Best is trial 11 with value: 0.02089347666583302.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.276917 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 18:51:44,839] Trial 12 finished with value: 0.020890916257698525 and parameters: {'num_leaves': 43, 'learning_rate': 0.12015566099107587, 'n_estimators': 157}. Best is trial 12 with value: 0.020890916257698525.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.185767 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 18:52:09,953] Trial 13 finished with value: 0.02141160646272519 and parameters: {'num_leaves': 34, 'learning_rate': 0.10440807667637056, 'n_estimators': 150}. Best is trial 12 with value: 0.020890916257698525.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.185538 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 18:52:35,961] Trial 14 finished with value: 0.032164747651574836 and parameters: {'num_leaves': 41, 'learning_rate': 0.032474141299282784, 'n_estimators': 140}. Best is trial 12 with value: 0.020890916257698525.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.185356 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 18:53:04,455] Trial 15 finished with value: 0.021325555057770708 and parameters: {'num_leaves': 44, 'learning_rate': 0.09079695459029855, 'n_estimators': 162}. Best is trial 12 with value: 0.020890916257698525.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.185683 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 18:53:22,394] Trial 16 finished with value: 0.02161884372907097 and parameters: {'num_leaves': 31, 'learning_rate': 0.2949025467087315, 'n_estimators': 134}. Best is trial 12 with value: 0.020890916257698525.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.183403 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 18:53:51,511] Trial 17 finished with value: 0.026717105921684442 and parameters: {'num_leaves': 39, 'learning_rate': 0.04501262359358996, 'n_estimators': 163}. Best is trial 12 with value: 0.020890916257698525.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.185042 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 18:54:27,802] Trial 18 finished with value: 0.034479597332558884 and parameters: {'num_leaves': 44, 'learning_rate': 0.01906960569560845, 'n_estimators': 191}. Best is trial 12 with value: 0.020890916257698525.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.186951 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 18:54:58,838] Trial 19 finished with value: 0.02152637756386614 and parameters: {'num_leaves': 38, 'learning_rate': 0.07828691808845226, 'n_estimators': 184}. Best is trial 12 with value: 0.020890916257698525.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.183263 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 18:55:23,361] Trial 20 finished with value: 0.020998064101158634 and parameters: {'num_leaves': 32, 'learning_rate': 0.18990264954370917, 'n_estimators': 165}. Best is trial 12 with value: 0.020890916257698525.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.183413 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 18:55:52,970] Trial 21 finished with value: 0.020857983720294525 and parameters: {'num_leaves': 45, 'learning_rate': 0.1566795738725457, 'n_estimators': 183}. Best is trial 21 with value: 0.020857983720294525.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.183592 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 18:56:24,794] Trial 22 finished with value: 0.020749942290897917 and parameters: {'num_leaves': 46, 'learning_rate': 0.13088492438703078, 'n_estimators': 188}. Best is trial 22 with value: 0.020749942290897917.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.185470 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 18:56:56,134] Trial 23 finished with value: 0.020803948889634785 and parameters: {'num_leaves': 45, 'learning_rate': 0.1398654927649905, 'n_estimators': 189}. Best is trial 22 with value: 0.020749942290897917.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.185958 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 18:57:26,415] Trial 24 finished with value: 0.02089277431888173 and parameters: {'num_leaves': 46, 'learning_rate': 0.15747197626595527, 'n_estimators': 188}. Best is trial 22 with value: 0.020749942290897917.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.186289 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 18:57:51,443] Trial 25 finished with value: 0.021304547920348676 and parameters: {'num_leaves': 46, 'learning_rate': 0.23191142383624647, 'n_estimators': 193}. Best is trial 22 with value: 0.020749942290897917.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.185159 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 18:58:22,590] Trial 26 finished with value: 0.02079700967580945 and parameters: {'num_leaves': 47, 'learning_rate': 0.14316654360268802, 'n_estimators': 186}. Best is trial 22 with value: 0.020749942290897917.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.183344 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 18:58:47,089] Trial 27 finished with value: 0.023854272259450558 and parameters: {'num_leaves': 20, 'learning_rate': 0.0726572724668407, 'n_estimators': 167}. Best is trial 22 with value: 0.020749942290897917.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.186379 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 18:59:21,352] Trial 28 finished with value: 0.02067463313473177 and parameters: {'num_leaves': 47, 'learning_rate': 0.12004101991019928, 'n_estimators': 199}. Best is trial 28 with value: 0.02067463313473177.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.183962 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 18:59:47,427] Trial 29 finished with value: 0.02127470659025018 and parameters: {'num_leaves': 48, 'learning_rate': 0.2249644988331908, 'n_estimators': 199}. Best is trial 28 with value: 0.02067463313473177.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.185848 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 19:00:22,837] Trial 30 finished with value: 0.023966473650586947 and parameters: {'num_leaves': 36, 'learning_rate': 0.04966789341740122, 'n_estimators': 200}. Best is trial 28 with value: 0.02067463313473177.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.453978 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 19:00:58,822] Trial 31 finished with value: 0.02074585337357016 and parameters: {'num_leaves': 47, 'learning_rate': 0.11938211368775507, 'n_estimators': 187}. Best is trial 28 with value: 0.02067463313473177.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.191614 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 19:01:31,810] Trial 32 finished with value: 0.020754527802599933 and parameters: {'num_leaves': 47, 'learning_rate': 0.10646235337589859, 'n_estimators': 185}. Best is trial 28 with value: 0.02067463313473177.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.188579 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 19:02:06,777] Trial 33 finished with value: 0.02069265082774473 and parameters: {'num_leaves': 50, 'learning_rate': 0.10295614166019752, 'n_estimators': 193}. Best is trial 28 with value: 0.02067463313473177.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.189869 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 19:02:42,273] Trial 34 finished with value: 0.021559951034935413 and parameters: {'num_leaves': 50, 'learning_rate': 0.06804401956639836, 'n_estimators': 194}. Best is trial 28 with value: 0.02067463313473177.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.187736 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 19:03:13,216] Trial 35 finished with value: 0.02124345595033898 and parameters: {'num_leaves': 50, 'learning_rate': 0.08660993597446347, 'n_estimators': 169}. Best is trial 28 with value: 0.02067463313473177.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.182703 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 19:03:47,786] Trial 36 finished with value: 0.02070646257668882 and parameters: {'num_leaves': 48, 'learning_rate': 0.11989911124186008, 'n_estimators': 197}. Best is trial 28 with value: 0.02067463313473177.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.189892 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 19:04:22,691] Trial 37 finished with value: 0.022803976093105142 and parameters: {'num_leaves': 40, 'learning_rate': 0.0564478358865575, 'n_estimators': 198}. Best is trial 28 with value: 0.02067463313473177.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.189379 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 19:04:54,998] Trial 38 finished with value: 0.020771223760646166 and parameters: {'num_leaves': 48, 'learning_rate': 0.11271965226934642, 'n_estimators': 180}. Best is trial 28 with value: 0.02067463313473177.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.186372 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 19:05:15,989] Trial 39 finished with value: 0.023717688119624037 and parameters: {'num_leaves': 27, 'learning_rate': 0.08676828506770991, 'n_estimators': 129}. Best is trial 28 with value: 0.02067463313473177.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.198061 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 19:05:53,434] Trial 40 finished with value: 0.025169524834494045 and parameters: {'num_leaves': 49, 'learning_rate': 0.04103566779624313, 'n_estimators': 195}. Best is trial 28 with value: 0.02067463313473177.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.191443 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 19:06:23,094] Trial 41 finished with value: 0.02097146286522861 and parameters: {'num_leaves': 48, 'learning_rate': 0.1713352451507202, 'n_estimators': 191}. Best is trial 28 with value: 0.02067463313473177.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.189990 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 19:06:58,099] Trial 42 finished with value: 0.02071648963088573 and parameters: {'num_leaves': 50, 'learning_rate': 0.1260572948043887, 'n_estimators': 200}. Best is trial 28 with value: 0.02067463313473177.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.185552 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 19:07:33,953] Trial 43 finished with value: 0.02071328711708446 and parameters: {'num_leaves': 50, 'learning_rate': 0.09646835612089205, 'n_estimators': 199}. Best is trial 28 with value: 0.02067463313473177.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.190850 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 19:08:10,842] Trial 44 finished with value: 0.02165644194142841 and parameters: {'num_leaves': 50, 'learning_rate': 0.06404036367504551, 'n_estimators': 200}. Best is trial 28 with value: 0.02067463313473177.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.187069 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 19:08:45,484] Trial 45 finished with value: 0.0207501507265077 and parameters: {'num_leaves': 49, 'learning_rate': 0.09493581243243125, 'n_estimators': 195}. Best is trial 28 with value: 0.02067463313473177.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.182980 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 19:09:15,785] Trial 46 finished with value: 0.020805112631867975 and parameters: {'num_leaves': 50, 'learning_rate': 0.12713032636312532, 'n_estimators': 172}. Best is trial 28 with value: 0.02067463313473177.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.184283 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 19:09:47,320] Trial 47 finished with value: 0.021473047893994145 and parameters: {'num_leaves': 43, 'learning_rate': 0.07909181897331183, 'n_estimators': 178}. Best is trial 28 with value: 0.02067463313473177.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.184292 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 19:10:08,144] Trial 48 finished with value: 0.021330231829190256 and parameters: {'num_leaves': 48, 'learning_rate': 0.21047581920154618, 'n_estimators': 121}. Best is trial 28 with value: 0.02067463313473177.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.184552 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 19:10:32,392] Trial 49 finished with value: 0.021410601007749337 and parameters: {'num_leaves': 45, 'learning_rate': 0.25444971811296624, 'n_estimators': 195}. Best is trial 28 with value: 0.02067463313473177.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.252988 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 19:11:08,642] Trial 50 finished with value: 0.04128256034925911 and parameters: {'num_leaves': 42, 'learning_rate': 0.010027618761627565, 'n_estimators': 181}. Best is trial 28 with value: 0.02067463313473177.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.184847 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 19:11:43,446] Trial 51 finished with value: 0.020693498926619876 and parameters: {'num_leaves': 47, 'learning_rate': 0.11593376898058502, 'n_estimators': 200}. Best is trial 28 with value: 0.02067463313473177.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.187197 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 19:12:17,457] Trial 52 finished with value: 0.02071593678294942 and parameters: {'num_leaves': 48, 'learning_rate': 0.10122016707039132, 'n_estimators': 192}. Best is trial 28 with value: 0.02067463313473177.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.186633 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 19:12:36,913] Trial 53 finished with value: 0.02306257719500662 and parameters: {'num_leaves': 48, 'learning_rate': 0.09912892118721932, 'n_estimators': 105}. Best is trial 28 with value: 0.02067463313473177.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.185331 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 19:13:05,837] Trial 54 finished with value: 0.02092761255358664 and parameters: {'num_leaves': 46, 'learning_rate': 0.1693376002090347, 'n_estimators': 192}. Best is trial 28 with value: 0.02067463313473177.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.185170 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 19:13:39,721] Trial 55 finished with value: 0.020743937776475017 and parameters: {'num_leaves': 43, 'learning_rate': 0.1001246544335083, 'n_estimators': 196}. Best is trial 28 with value: 0.02067463313473177.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.190950 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 19:14:14,112] Trial 56 finished with value: 0.021207208368992182 and parameters: {'num_leaves': 49, 'learning_rate': 0.0763643951064211, 'n_estimators': 191}. Best is trial 28 with value: 0.02067463313473177.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.201788 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 19:14:40,961] Trial 57 finished with value: 0.02106841748168969 and parameters: {'num_leaves': 44, 'learning_rate': 0.11215754693591391, 'n_estimators': 152}. Best is trial 28 with value: 0.02067463313473177.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.188044 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 19:15:11,782] Trial 58 finished with value: 0.02081625723854455 and parameters: {'num_leaves': 47, 'learning_rate': 0.14977134405878828, 'n_estimators': 184}. Best is trial 28 with value: 0.02067463313473177.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.186644 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 19:15:49,436] Trial 59 finished with value: 0.03316156737938884 and parameters: {'num_leaves': 45, 'learning_rate': 0.02067032780614252, 'n_estimators': 197}. Best is trial 28 with value: 0.02067463313473177.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.189019 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 19:16:21,187] Trial 60 finished with value: 0.02260375593572956 and parameters: {'num_leaves': 49, 'learning_rate': 0.06328161429210222, 'n_estimators': 172}. Best is trial 28 with value: 0.02067463313473177.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.185123 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 19:16:55,074] Trial 61 finished with value: 0.020756786593740365 and parameters: {'num_leaves': 50, 'learning_rate': 0.1329932515716508, 'n_estimators': 200}. Best is trial 28 with value: 0.02067463313473177.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.187647 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 19:17:22,301] Trial 62 finished with value: 0.021081479312219858 and parameters: {'num_leaves': 47, 'learning_rate': 0.18923324176347844, 'n_estimators': 190}. Best is trial 28 with value: 0.02067463313473177.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.184089 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 19:17:56,615] Trial 63 finished with value: 0.020700108867149493 and parameters: {'num_leaves': 49, 'learning_rate': 0.12014917542327659, 'n_estimators': 197}. Best is trial 28 with value: 0.02067463313473177.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.183296 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 19:18:30,747] Trial 64 finished with value: 0.020867225799721483 and parameters: {'num_leaves': 46, 'learning_rate': 0.0892662468099933, 'n_estimators': 193}. Best is trial 28 with value: 0.02067463313473177.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.185502 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 19:19:03,796] Trial 65 finished with value: 0.02073475951302726 and parameters: {'num_leaves': 48, 'learning_rate': 0.1183200045892609, 'n_estimators': 188}. Best is trial 28 with value: 0.02067463313473177.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.226553 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 19:19:39,242] Trial 66 finished with value: 0.02070439351505224 and parameters: {'num_leaves': 49, 'learning_rate': 0.10571945139169081, 'n_estimators': 198}. Best is trial 28 with value: 0.02067463313473177.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.189396 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 19:20:11,916] Trial 67 finished with value: 0.020770088735259373 and parameters: {'num_leaves': 49, 'learning_rate': 0.1447025041140858, 'n_estimators': 197}. Best is trial 28 with value: 0.02067463313473177.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.185103 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 19:20:42,126] Trial 68 finished with value: 0.02088423604648259 and parameters: {'num_leaves': 44, 'learning_rate': 0.16490281762265235, 'n_estimators': 197}. Best is trial 28 with value: 0.02067463313473177.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.183448 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 19:21:14,045] Trial 69 finished with value: 0.02078451296043129 and parameters: {'num_leaves': 46, 'learning_rate': 0.10847736894005197, 'n_estimators': 182}. Best is trial 28 with value: 0.02067463313473177.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.191412 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 19:21:44,083] Trial 70 finished with value: 0.020719289030127156 and parameters: {'num_leaves': 36, 'learning_rate': 0.13566702584966606, 'n_estimators': 186}. Best is trial 28 with value: 0.02067463313473177.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.269222 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 19:22:18,750] Trial 71 finished with value: 0.020794056307458262 and parameters: {'num_leaves': 49, 'learning_rate': 0.0945937200121135, 'n_estimators': 193}. Best is trial 28 with value: 0.02067463313473177.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.184073 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 19:22:52,662] Trial 72 finished with value: 0.02104267999467486 and parameters: {'num_leaves': 47, 'learning_rate': 0.08222204934966977, 'n_estimators': 190}. Best is trial 28 with value: 0.02067463313473177.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.186263 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 19:23:28,117] Trial 73 finished with value: 0.021471411413268124 and parameters: {'num_leaves': 48, 'learning_rate': 0.06906694981450115, 'n_estimators': 197}. Best is trial 28 with value: 0.02067463313473177.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.184008 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 19:24:02,604] Trial 74 finished with value: 0.02070976700574646 and parameters: {'num_leaves': 50, 'learning_rate': 0.10296980899583413, 'n_estimators': 193}. Best is trial 28 with value: 0.02067463313473177.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.180226 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 19:24:36,843] Trial 75 finished with value: 0.02069151889288518 and parameters: {'num_leaves': 49, 'learning_rate': 0.11622264792208192, 'n_estimators': 194}. Best is trial 28 with value: 0.02067463313473177.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.183768 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 19:24:58,319] Trial 76 finished with value: 0.0216820746330812 and parameters: {'num_leaves': 23, 'learning_rate': 0.11802800436266637, 'n_estimators': 141}. Best is trial 28 with value: 0.02067463313473177.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.185865 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 19:25:28,583] Trial 77 finished with value: 0.020838880113824558 and parameters: {'num_leaves': 49, 'learning_rate': 0.15670418859150814, 'n_estimators': 188}. Best is trial 28 with value: 0.02067463313473177.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.262568 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 19:25:55,840] Trial 78 finished with value: 0.021087295342141786 and parameters: {'num_leaves': 46, 'learning_rate': 0.19275628400247538, 'n_estimators': 195}. Best is trial 28 with value: 0.02067463313473177.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.183396 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 19:26:26,485] Trial 79 finished with value: 0.02076477565900581 and parameters: {'num_leaves': 45, 'learning_rate': 0.12588394076317394, 'n_estimators': 179}. Best is trial 28 with value: 0.02067463313473177.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.182493 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 19:26:58,869] Trial 80 finished with value: 0.02071968301670216 and parameters: {'num_leaves': 47, 'learning_rate': 0.10904267620409971, 'n_estimators': 185}. Best is trial 28 with value: 0.02067463313473177.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.182622 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 19:27:34,444] Trial 81 finished with value: 0.020805854140077944 and parameters: {'num_leaves': 50, 'learning_rate': 0.08688808935207333, 'n_estimators': 200}. Best is trial 28 with value: 0.02067463313473177.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.185945 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 19:28:07,297] Trial 82 finished with value: 0.020784543397335927 and parameters: {'num_leaves': 50, 'learning_rate': 0.14093546355733422, 'n_estimators': 198}. Best is trial 28 with value: 0.02067463313473177.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.186260 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 19:28:42,266] Trial 83 finished with value: 0.02124224412830002 and parameters: {'num_leaves': 49, 'learning_rate': 0.07442677602783505, 'n_estimators': 194}. Best is trial 28 with value: 0.02067463313473177.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.187232 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 19:29:16,108] Trial 84 finished with value: 0.02081461626386402 and parameters: {'num_leaves': 50, 'learning_rate': 0.09338118554677574, 'n_estimators': 189}. Best is trial 28 with value: 0.02067463313473177.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.184948 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 19:29:50,693] Trial 85 finished with value: 0.020686110291138152 and parameters: {'num_leaves': 48, 'learning_rate': 0.10527897373499029, 'n_estimators': 198}. Best is trial 28 with value: 0.02067463313473177.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.186064 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 19:30:24,371] Trial 86 finished with value: 0.020700352146434488 and parameters: {'num_leaves': 48, 'learning_rate': 0.11868663763599265, 'n_estimators': 194}. Best is trial 28 with value: 0.02067463313473177.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.189198 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 19:30:55,828] Trial 87 finished with value: 0.020682388367582093 and parameters: {'num_leaves': 33, 'learning_rate': 0.11896877949729956, 'n_estimators': 197}. Best is trial 28 with value: 0.02067463313473177.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.185387 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 19:31:25,400] Trial 88 finished with value: 0.020788906346954412 and parameters: {'num_leaves': 31, 'learning_rate': 0.15160438872049864, 'n_estimators': 191}. Best is trial 28 with value: 0.02067463313473177.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.186662 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 19:32:02,495] Trial 89 finished with value: 0.03884099821610325 and parameters: {'num_leaves': 33, 'learning_rate': 0.013182224340720701, 'n_estimators': 195}. Best is trial 28 with value: 0.02067463313473177.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.191235 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 19:32:34,135] Trial 90 finished with value: 0.020680059642307028 and parameters: {'num_leaves': 34, 'learning_rate': 0.12990139031057166, 'n_estimators': 200}. Best is trial 28 with value: 0.02067463313473177.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.214578 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 19:33:05,357] Trial 91 finished with value: 0.020703566093114006 and parameters: {'num_leaves': 31, 'learning_rate': 0.1330406421710388, 'n_estimators': 200}. Best is trial 28 with value: 0.02067463313473177.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.185453 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 19:33:36,193] Trial 92 finished with value: 0.02067669094173632 and parameters: {'num_leaves': 29, 'learning_rate': 0.1311288862760473, 'n_estimators': 200}. Best is trial 28 with value: 0.02067463313473177.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.187814 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 19:34:06,031] Trial 93 finished with value: 0.020711174574215795 and parameters: {'num_leaves': 27, 'learning_rate': 0.11654460344010993, 'n_estimators': 195}. Best is trial 28 with value: 0.02067463313473177.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.187101 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 19:34:37,504] Trial 94 finished with value: 0.020667064181875396 and parameters: {'num_leaves': 34, 'learning_rate': 0.1282998099778596, 'n_estimators': 198}. Best is trial 94 with value: 0.020667064181875396.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.184527 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 19:35:03,866] Trial 95 finished with value: 0.02088577616118408 and parameters: {'num_leaves': 35, 'learning_rate': 0.12707857534281725, 'n_estimators': 160}. Best is trial 94 with value: 0.020667064181875396.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.190279 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 19:35:31,315] Trial 96 finished with value: 0.020897472593825562 and parameters: {'num_leaves': 33, 'learning_rate': 0.18140850701286218, 'n_estimators': 198}. Best is trial 94 with value: 0.020667064181875396.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.185099 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 19:36:00,410] Trial 97 finished with value: 0.02079108871283696 and parameters: {'num_leaves': 29, 'learning_rate': 0.16205698154130888, 'n_estimators': 200}. Best is trial 94 with value: 0.020667064181875396.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.186086 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 19:36:31,603] Trial 98 finished with value: 0.020784400855259354 and parameters: {'num_leaves': 37, 'learning_rate': 0.141205853643728, 'n_estimators': 191}. Best is trial 94 with value: 0.020667064181875396.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.183905 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-05-08 19:37:02,808] Trial 99 finished with value: 0.021360684860082874 and parameters: {'num_leaves': 30, 'learning_rate': 0.08200596044772845, 'n_estimators': 196}. Best is trial 94 with value: 0.020667064181875396.\n","output_type":"stream"},{"name":"stdout","text":"{'num_leaves': 34, 'learning_rate': 0.1282998099778596, 'n_estimators': 198}\n","output_type":"stream"}]},{"cell_type":"code","source":"lgb_model = lgb.LGBMRegressor(num_leaves=34, learning_rate=0.1282998099778596, n_estimators=198)\nlgb_model = lgb_model.fit(X_train, y_train)\n\n#test set preds\nlgb_preds = tensor(lgb_model.predict(test_dl.xs))\n\n#validation set preds\nlgb_preds_x = tensor(lgb_model.predict(X_test))\n\n\nr2_score(y_test,lgb_preds_x)","metadata":{"execution":{"iopub.status.busy":"2024-05-08T20:22:57.243371Z","iopub.execute_input":"2024-05-08T20:22:57.243962Z","iopub.status.idle":"2024-05-08T20:23:37.839499Z","shell.execute_reply.started":"2024-05-08T20:22:57.243894Z","shell.execute_reply":"2024-05-08T20:23:37.838207Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.197787 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"execution_count":47,"output_type":"execute_result","data":{"text/plain":"0.8361207633995348"},"metadata":{}}]},{"cell_type":"code","source":"lgb_model = lgb.LGBMRegressor(num_leaves=47, learning_rate=0.19470589857022183, n_estimators=165)\nlgb_model = lgb_model.fit(X_train, y_train)\n\n#test set preds\nlgb_preds = tensor(lgb_model.predict(test_dl.xs))\n\n#validation set preds\nlgb_preds_x = tensor(lgb_model.predict(X_test))\n\n\nr2_score(y_test,lgb_preds_x)","metadata":{"execution":{"iopub.status.busy":"2024-05-08T17:30:39.314446Z","iopub.execute_input":"2024-05-08T17:30:39.315421Z","iopub.status.idle":"2024-05-08T17:31:15.896121Z","shell.execute_reply.started":"2024-05-08T17:30:39.315372Z","shell.execute_reply":"2024-05-08T17:31:15.89478Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.276110 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 368\n[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 20\n[LightGBM] [Info] Start training from score 0.504235\n","output_type":"stream"},{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"0.8289420933149213"},"metadata":{}}]},{"cell_type":"markdown","source":"# XGBoost","metadata":{}},{"cell_type":"code","source":"def objective_xgboost(trial):\n    params = {\n        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 200),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 1, 10),\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n        \"subsample\": trial.suggest_float(\"subsample\", 0.1, 1.0),\n    }\n    model = xgb.XGBRegressor(**params)\n    model.fit(X_train, y_train)\n    xgb_predictions = model.predict(X_test)\n    rmse = mean_squared_error(y_test, xgb_predictions, squared=False)\n    return rmse\n\nstudy_xgboost = optuna.create_study(direction='minimize')\nstudy_xgboost.optimize(objective_xgboost, n_trials=100)\nprint(study_xgboost.best_params)","metadata":{"execution":{"iopub.status.busy":"2024-05-08T19:37:02.820761Z","iopub.execute_input":"2024-05-08T19:37:02.821546Z","iopub.status.idle":"2024-05-08T20:19:44.287066Z","shell.execute_reply.started":"2024-05-08T19:37:02.821486Z","shell.execute_reply":"2024-05-08T20:19:44.285819Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stderr","text":"[I 2024-05-08 19:37:02,829] A new study created in memory with name: no-name-ffe12cb2-29d9-47a6-a404-445c5102b28b\n[I 2024-05-08 19:37:20,561] Trial 0 finished with value: 0.021343758329749107 and parameters: {'n_estimators': 176, 'max_depth': 5, 'learning_rate': 0.27308559052143444, 'subsample': 0.7894496370597744}. Best is trial 0 with value: 0.021343758329749107.\n[I 2024-05-08 19:37:35,314] Trial 1 finished with value: 0.030077384784817696 and parameters: {'n_estimators': 128, 'max_depth': 4, 'learning_rate': 0.06667131469377875, 'subsample': 0.631565489994721}. Best is trial 0 with value: 0.021343758329749107.\n[I 2024-05-08 19:38:02,201] Trial 2 finished with value: 0.036746107041835785 and parameters: {'n_estimators': 163, 'max_depth': 6, 'learning_rate': 0.017316193973622957, 'subsample': 0.27453922283558374}. Best is trial 0 with value: 0.021343758329749107.\n[I 2024-05-08 19:38:14,508] Trial 3 finished with value: 0.030679412186145782 and parameters: {'n_estimators': 128, 'max_depth': 3, 'learning_rate': 0.08331283718449431, 'subsample': 0.3574521410987146}. Best is trial 0 with value: 0.021343758329749107.\n[I 2024-05-08 19:38:41,239] Trial 4 finished with value: 0.037361398339271545 and parameters: {'n_estimators': 164, 'max_depth': 6, 'learning_rate': 0.016678770348144127, 'subsample': 0.8950209992554361}. Best is trial 0 with value: 0.021343758329749107.\n[I 2024-05-08 19:39:16,384] Trial 5 finished with value: 0.02120213955640793 and parameters: {'n_estimators': 180, 'max_depth': 10, 'learning_rate': 0.07982786375406618, 'subsample': 0.361038794716779}. Best is trial 5 with value: 0.02120213955640793.\n[I 2024-05-08 19:39:46,449] Trial 6 finished with value: 0.03164990246295929 and parameters: {'n_estimators': 197, 'max_depth': 6, 'learning_rate': 0.023940652798246257, 'subsample': 0.28031758055707146}. Best is trial 5 with value: 0.02120213955640793.\n[I 2024-05-08 19:40:01,147] Trial 7 finished with value: 0.028942931443452835 and parameters: {'n_estimators': 153, 'max_depth': 3, 'learning_rate': 0.08266399288290567, 'subsample': 0.4595392155915007}. Best is trial 5 with value: 0.02120213955640793.\n[I 2024-05-08 19:40:16,224] Trial 8 finished with value: 0.043792884796857834 and parameters: {'n_estimators': 182, 'max_depth': 2, 'learning_rate': 0.019568113044059126, 'subsample': 0.20755174402062876}. Best is trial 5 with value: 0.02120213955640793.\n[I 2024-05-08 19:40:32,570] Trial 9 finished with value: 0.02972228080034256 and parameters: {'n_estimators': 122, 'max_depth': 5, 'learning_rate': 0.05799062556615767, 'subsample': 0.7495845067959817}. Best is trial 5 with value: 0.02120213955640793.\n[I 2024-05-08 19:40:49,496] Trial 10 finished with value: 0.02373795211315155 and parameters: {'n_estimators': 101, 'max_depth': 10, 'learning_rate': 0.1982984111659432, 'subsample': 0.1050788405912722}. Best is trial 5 with value: 0.02120213955640793.\n[I 2024-05-08 19:41:19,052] Trial 11 finished with value: 0.022162841632962227 and parameters: {'n_estimators': 183, 'max_depth': 9, 'learning_rate': 0.26943354690259674, 'subsample': 0.9830407175520903}. Best is trial 5 with value: 0.02120213955640793.\n[I 2024-05-08 19:41:46,568] Trial 12 finished with value: 0.02119792252779007 and parameters: {'n_estimators': 181, 'max_depth': 8, 'learning_rate': 0.13392393238029046, 'subsample': 0.579761843147556}. Best is trial 12 with value: 0.02119792252779007.\n[I 2024-05-08 19:42:15,351] Trial 13 finished with value: 0.02118554711341858 and parameters: {'n_estimators': 194, 'max_depth': 8, 'learning_rate': 0.13237250650690086, 'subsample': 0.5454136674720665}. Best is trial 13 with value: 0.02118554711341858.\n[I 2024-05-08 19:42:44,608] Trial 14 finished with value: 0.02117026038467884 and parameters: {'n_estimators': 200, 'max_depth': 8, 'learning_rate': 0.13138999296167353, 'subsample': 0.567633263658979}. Best is trial 14 with value: 0.02117026038467884.\n[I 2024-05-08 19:43:14,383] Trial 15 finished with value: 0.021123971790075302 and parameters: {'n_estimators': 200, 'max_depth': 8, 'learning_rate': 0.12542347851677518, 'subsample': 0.4933207838596616}. Best is trial 15 with value: 0.021123971790075302.\n[I 2024-05-08 19:43:49,554] Trial 16 finished with value: 0.02576463855803013 and parameters: {'n_estimators': 195, 'max_depth': 8, 'learning_rate': 0.03314682632604965, 'subsample': 0.6844369848895288}. Best is trial 15 with value: 0.021123971790075302.\n[I 2024-05-08 19:44:16,913] Trial 17 finished with value: 0.021038994193077087 and parameters: {'n_estimators': 200, 'max_depth': 7, 'learning_rate': 0.1318658176260804, 'subsample': 0.47358604749916966}. Best is trial 17 with value: 0.021038994193077087.\n[I 2024-05-08 19:44:41,823] Trial 18 finished with value: 0.028789209201931953 and parameters: {'n_estimators': 147, 'max_depth': 7, 'learning_rate': 0.036563996821678275, 'subsample': 0.46501859613051105}. Best is trial 17 with value: 0.021038994193077087.\n[I 2024-05-08 19:44:52,407] Trial 19 finished with value: 0.04909447953104973 and parameters: {'n_estimators': 165, 'max_depth': 1, 'learning_rate': 0.010138983167009155, 'subsample': 0.4541073421109803}. Best is trial 17 with value: 0.021038994193077087.\n[I 2024-05-08 19:45:12,047] Trial 20 finished with value: 0.0214194618165493 and parameters: {'n_estimators': 141, 'max_depth': 7, 'learning_rate': 0.16324831396095923, 'subsample': 0.3865077326026191}. Best is trial 17 with value: 0.021038994193077087.\n[I 2024-05-08 19:45:44,781] Trial 21 finished with value: 0.021201398223638535 and parameters: {'n_estimators': 198, 'max_depth': 9, 'learning_rate': 0.11399844277881348, 'subsample': 0.5233077119286376}. Best is trial 17 with value: 0.021038994193077087.\n[I 2024-05-08 19:46:08,687] Trial 22 finished with value: 0.021475017070770264 and parameters: {'n_estimators': 190, 'max_depth': 7, 'learning_rate': 0.2024852021720396, 'subsample': 0.6361411039425888}. Best is trial 17 with value: 0.021038994193077087.\n[I 2024-05-08 19:46:42,793] Trial 23 finished with value: 0.021038325503468513 and parameters: {'n_estimators': 200, 'max_depth': 9, 'learning_rate': 0.10117226354251943, 'subsample': 0.7284268842327694}. Best is trial 23 with value: 0.021038325503468513.\n[I 2024-05-08 19:47:14,882] Trial 24 finished with value: 0.021058594807982445 and parameters: {'n_estimators': 188, 'max_depth': 9, 'learning_rate': 0.10366245460049442, 'subsample': 0.7939266817192784}. Best is trial 23 with value: 0.021038325503468513.\n[I 2024-05-08 19:47:48,615] Trial 25 finished with value: 0.02401003986597061 and parameters: {'n_estimators': 172, 'max_depth': 9, 'learning_rate': 0.04195537552922419, 'subsample': 0.8461494709027355}. Best is trial 23 with value: 0.021038325503468513.\n[I 2024-05-08 19:48:24,548] Trial 26 finished with value: 0.0211749579757452 and parameters: {'n_estimators': 189, 'max_depth': 10, 'learning_rate': 0.09937408044491453, 'subsample': 0.7194235569407075}. Best is trial 23 with value: 0.021038325503468513.\n[I 2024-05-08 19:49:00,231] Trial 27 finished with value: 0.022414671257138252 and parameters: {'n_estimators': 187, 'max_depth': 9, 'learning_rate': 0.05032792094941324, 'subsample': 0.9096324396607715}. Best is trial 23 with value: 0.021038325503468513.\n[I 2024-05-08 19:49:22,843] Trial 28 finished with value: 0.021331124007701874 and parameters: {'n_estimators': 173, 'max_depth': 7, 'learning_rate': 0.18257939875416246, 'subsample': 0.7999153690115761}. Best is trial 23 with value: 0.021038325503468513.\n[I 2024-05-08 19:49:50,885] Trial 29 finished with value: 0.02219516970217228 and parameters: {'n_estimators': 175, 'max_depth': 9, 'learning_rate': 0.246184280208786, 'subsample': 0.7930744499304625}. Best is trial 23 with value: 0.021038325503468513.\n[I 2024-05-08 19:50:27,244] Trial 30 finished with value: 0.021190200001001358 and parameters: {'n_estimators': 188, 'max_depth': 10, 'learning_rate': 0.10008981202676626, 'subsample': 0.6783831445075541}. Best is trial 23 with value: 0.021038325503468513.\n[I 2024-05-08 19:50:55,610] Trial 31 finished with value: 0.021359525620937347 and parameters: {'n_estimators': 200, 'max_depth': 8, 'learning_rate': 0.15709443054697214, 'subsample': 0.6315288056552215}. Best is trial 23 with value: 0.021038325503468513.\n[I 2024-05-08 19:51:22,983] Trial 32 finished with value: 0.021014628931879997 and parameters: {'n_estimators': 192, 'max_depth': 7, 'learning_rate': 0.10518506142333259, 'subsample': 0.4894196269556319}. Best is trial 32 with value: 0.021014628931879997.\n[I 2024-05-08 19:51:46,332] Trial 33 finished with value: 0.023847412317991257 and parameters: {'n_estimators': 191, 'max_depth': 5, 'learning_rate': 0.06794341161325528, 'subsample': 0.3996317900861945}. Best is trial 32 with value: 0.021014628931879997.\n[I 2024-05-08 19:52:08,912] Trial 34 finished with value: 0.021631671115756035 and parameters: {'n_estimators': 169, 'max_depth': 6, 'learning_rate': 0.0979374018518954, 'subsample': 0.6228548127438618}. Best is trial 32 with value: 0.021014628931879997.\n[I 2024-05-08 19:52:28,896] Trial 35 finished with value: 0.026102665811777115 and parameters: {'n_estimators': 187, 'max_depth': 4, 'learning_rate': 0.06791579488798623, 'subsample': 0.8652735310384106}. Best is trial 32 with value: 0.021014628931879997.\n[I 2024-05-08 19:52:52,519] Trial 36 finished with value: 0.022023383527994156 and parameters: {'n_estimators': 157, 'max_depth': 7, 'learning_rate': 0.08353635543408576, 'subsample': 0.9998075137404738}. Best is trial 32 with value: 0.021014628931879997.\n[I 2024-05-08 19:53:17,285] Trial 37 finished with value: 0.02526186779141426 and parameters: {'n_estimators': 178, 'max_depth': 6, 'learning_rate': 0.05218148580407627, 'subsample': 0.9284520067230965}. Best is trial 32 with value: 0.021014628931879997.\n[I 2024-05-08 19:53:36,547] Trial 38 finished with value: 0.021066542714834213 and parameters: {'n_estimators': 195, 'max_depth': 4, 'learning_rate': 0.14923621148288577, 'subsample': 0.7555484020873199}. Best is trial 32 with value: 0.021014628931879997.\n[I 2024-05-08 19:54:07,122] Trial 39 finished with value: 0.021277057006955147 and parameters: {'n_estimators': 184, 'max_depth': 9, 'learning_rate': 0.10663827152170167, 'subsample': 0.2983061478250452}. Best is trial 32 with value: 0.021014628931879997.\n[I 2024-05-08 19:54:22,366] Trial 40 finished with value: 0.021379413083195686 and parameters: {'n_estimators': 135, 'max_depth': 5, 'learning_rate': 0.22511964592975137, 'subsample': 0.4153656374928374}. Best is trial 32 with value: 0.021014628931879997.\n[I 2024-05-08 19:54:42,231] Trial 41 finished with value: 0.023940669372677803 and parameters: {'n_estimators': 194, 'max_depth': 4, 'learning_rate': 0.08346348776735842, 'subsample': 0.7498363594257632}. Best is trial 32 with value: 0.021014628931879997.\n[I 2024-05-08 19:54:59,559] Trial 42 finished with value: 0.021867359057068825 and parameters: {'n_estimators': 193, 'max_depth': 3, 'learning_rate': 0.15380103386969704, 'subsample': 0.8507758456400812}. Best is trial 32 with value: 0.021014628931879997.\n[I 2024-05-08 19:55:19,289] Trial 43 finished with value: 0.02140107937157154 and parameters: {'n_estimators': 195, 'max_depth': 4, 'learning_rate': 0.2992808558659145, 'subsample': 0.3303061293890567}. Best is trial 32 with value: 0.021014628931879997.\n[I 2024-05-08 19:55:37,058] Trial 44 finished with value: 0.021924009546637535 and parameters: {'n_estimators': 180, 'max_depth': 3, 'learning_rate': 0.1653575185060621, 'subsample': 0.7519268533415233}. Best is trial 32 with value: 0.021014628931879997.\n[I 2024-05-08 19:55:52,701] Trial 45 finished with value: 0.027526704594492912 and parameters: {'n_estimators': 185, 'max_depth': 2, 'learning_rate': 0.11750762911511907, 'subsample': 0.8160130935356564}. Best is trial 32 with value: 0.021014628931879997.\n[I 2024-05-08 19:56:13,069] Trial 46 finished with value: 0.02767912857234478 and parameters: {'n_estimators': 118, 'max_depth': 6, 'learning_rate': 0.060834095813075674, 'subsample': 0.6998464497626762}. Best is trial 32 with value: 0.021014628931879997.\n[I 2024-05-08 19:56:52,000] Trial 47 finished with value: 0.021111471578478813 and parameters: {'n_estimators': 192, 'max_depth': 10, 'learning_rate': 0.07530804338571856, 'subsample': 0.5983548068753353}. Best is trial 32 with value: 0.021014628931879997.\n[I 2024-05-08 19:57:14,110] Trial 48 finished with value: 0.020923776552081108 and parameters: {'n_estimators': 197, 'max_depth': 5, 'learning_rate': 0.13948395821213827, 'subsample': 0.49446642863570467}. Best is trial 48 with value: 0.020923776552081108.\n[I 2024-05-08 19:57:40,874] Trial 49 finished with value: 0.0212630033493042 and parameters: {'n_estimators': 200, 'max_depth': 6, 'learning_rate': 0.09116361006288827, 'subsample': 0.5387067065404364}. Best is trial 48 with value: 0.020923776552081108.\n[I 2024-05-08 19:58:01,102] Trial 50 finished with value: 0.021385200321674347 and parameters: {'n_estimators': 159, 'max_depth': 5, 'learning_rate': 0.13716063544004048, 'subsample': 0.49611499451497415}. Best is trial 48 with value: 0.020923776552081108.\n[I 2024-05-08 19:58:23,909] Trial 51 finished with value: 0.021897045895457268 and parameters: {'n_estimators': 196, 'max_depth': 4, 'learning_rate': 0.11451201915028704, 'subsample': 0.451056453794905}. Best is trial 48 with value: 0.020923776552081108.\n[I 2024-05-08 19:58:48,483] Trial 52 finished with value: 0.020857330411672592 and parameters: {'n_estimators': 196, 'max_depth': 5, 'learning_rate': 0.1461774202844157, 'subsample': 0.6649609199174655}. Best is trial 52 with value: 0.020857330411672592.\n[I 2024-05-08 19:59:10,242] Trial 53 finished with value: 0.021082069724798203 and parameters: {'n_estimators': 184, 'max_depth': 5, 'learning_rate': 0.21352354091681666, 'subsample': 0.6559719366984543}. Best is trial 52 with value: 0.020857330411672592.\n[I 2024-05-08 19:59:36,332] Trial 54 finished with value: 0.021318159997463226 and parameters: {'n_estimators': 191, 'max_depth': 7, 'learning_rate': 0.17678262634263275, 'subsample': 0.591300770538208}. Best is trial 52 with value: 0.020857330411672592.\n[I 2024-05-08 20:00:05,369] Trial 55 finished with value: 0.021210158243775368 and parameters: {'n_estimators': 198, 'max_depth': 8, 'learning_rate': 0.1334883088488739, 'subsample': 0.5047217849349407}. Best is trial 52 with value: 0.020857330411672592.\n[I 2024-05-08 20:00:31,061] Trial 56 finished with value: 0.022141102701425552 and parameters: {'n_estimators': 188, 'max_depth': 6, 'learning_rate': 0.07624535195565947, 'subsample': 0.5531267354537859}. Best is trial 52 with value: 0.020857330411672592.\n[I 2024-05-08 20:00:43,478] Trial 57 finished with value: 0.02214939333498478 and parameters: {'n_estimators': 102, 'max_depth': 5, 'learning_rate': 0.18872195764798916, 'subsample': 0.41845233350632505}. Best is trial 52 with value: 0.020857330411672592.\n[I 2024-05-08 20:01:12,016] Trial 58 finished with value: 0.021120550110936165 and parameters: {'n_estimators': 178, 'max_depth': 8, 'learning_rate': 0.09367551470662477, 'subsample': 0.36707909353539414}. Best is trial 52 with value: 0.020857330411672592.\n[I 2024-05-08 20:01:39,240] Trial 59 finished with value: 0.02104579284787178 and parameters: {'n_estimators': 197, 'max_depth': 7, 'learning_rate': 0.14330999614417214, 'subsample': 0.7174164400242757}. Best is trial 52 with value: 0.020857330411672592.\n[I 2024-05-08 20:02:09,910] Trial 60 finished with value: 0.02099759690463543 and parameters: {'n_estimators': 197, 'max_depth': 7, 'learning_rate': 0.14142035777959244, 'subsample': 0.7132089798731135}. Best is trial 52 with value: 0.020857330411672592.\n[I 2024-05-08 20:02:40,803] Trial 61 finished with value: 0.021021654829382896 and parameters: {'n_estimators': 197, 'max_depth': 7, 'learning_rate': 0.1402370875256388, 'subsample': 0.7078094283351557}. Best is trial 52 with value: 0.020857330411672592.\n[I 2024-05-08 20:03:12,595] Trial 62 finished with value: 0.020934661850333214 and parameters: {'n_estimators': 197, 'max_depth': 7, 'learning_rate': 0.12361040572742513, 'subsample': 0.6662744230828049}. Best is trial 52 with value: 0.020857330411672592.\n[I 2024-05-08 20:03:40,476] Trial 63 finished with value: 0.02094435878098011 and parameters: {'n_estimators': 193, 'max_depth': 6, 'learning_rate': 0.120750599566805, 'subsample': 0.6532320340402331}. Best is trial 52 with value: 0.020857330411672592.\n[I 2024-05-08 20:04:08,858] Trial 64 finished with value: 0.020957699045538902 and parameters: {'n_estimators': 193, 'max_depth': 6, 'learning_rate': 0.11386869574236295, 'subsample': 0.6691266585316472}. Best is trial 52 with value: 0.020857330411672592.\n[I 2024-05-08 20:04:37,383] Trial 65 finished with value: 0.02096611261367798 and parameters: {'n_estimators': 191, 'max_depth': 6, 'learning_rate': 0.11783885360710905, 'subsample': 0.6787127894099018}. Best is trial 52 with value: 0.020857330411672592.\n[I 2024-05-08 20:05:04,512] Trial 66 finished with value: 0.021032104268670082 and parameters: {'n_estimators': 182, 'max_depth': 6, 'learning_rate': 0.12117488671577138, 'subsample': 0.6639181981347818}. Best is trial 52 with value: 0.020857330411672592.\n[I 2024-05-08 20:05:30,688] Trial 67 finished with value: 0.02107793278992176 and parameters: {'n_estimators': 185, 'max_depth': 6, 'learning_rate': 0.17555456905359143, 'subsample': 0.6074631991593398}. Best is trial 52 with value: 0.020857330411672592.\n[I 2024-05-08 20:05:53,939] Trial 68 finished with value: 0.02119169756770134 and parameters: {'n_estimators': 191, 'max_depth': 5, 'learning_rate': 0.22959567886908863, 'subsample': 0.656606181457429}. Best is trial 52 with value: 0.020857330411672592.\n[I 2024-05-08 20:06:22,776] Trial 69 finished with value: 0.020936476066708565 and parameters: {'n_estimators': 195, 'max_depth': 6, 'learning_rate': 0.12577395260452487, 'subsample': 0.5802440946253744}. Best is trial 52 with value: 0.020857330411672592.\n[I 2024-05-08 20:06:56,875] Trial 70 finished with value: 0.031846120953559875 and parameters: {'n_estimators': 193, 'max_depth': 6, 'learning_rate': 0.024595384752586925, 'subsample': 0.6290754232020203}. Best is trial 52 with value: 0.020857330411672592.\n[I 2024-05-08 20:07:25,880] Trial 71 finished with value: 0.020912282168865204 and parameters: {'n_estimators': 197, 'max_depth': 6, 'learning_rate': 0.12400995869824172, 'subsample': 0.5742025607064376}. Best is trial 52 with value: 0.020857330411672592.\n[I 2024-05-08 20:07:54,153] Trial 72 finished with value: 0.02098819613456726 and parameters: {'n_estimators': 189, 'max_depth': 6, 'learning_rate': 0.11730497392531702, 'subsample': 0.571759600583839}. Best is trial 52 with value: 0.020857330411672592.\n[I 2024-05-08 20:08:19,628] Trial 73 finished with value: 0.02102457359433174 and parameters: {'n_estimators': 194, 'max_depth': 5, 'learning_rate': 0.12661227572304082, 'subsample': 0.5334244389500973}. Best is trial 52 with value: 0.020857330411672592.\n[I 2024-05-08 20:08:46,707] Trial 74 finished with value: 0.022021831944584846 and parameters: {'n_estimators': 200, 'max_depth': 5, 'learning_rate': 0.08774544323293258, 'subsample': 0.6769889235808144}. Best is trial 52 with value: 0.020857330411672592.\n[I 2024-05-08 20:09:15,287] Trial 75 finished with value: 0.02108119986951351 and parameters: {'n_estimators': 190, 'max_depth': 6, 'learning_rate': 0.1091827031358797, 'subsample': 0.5767835610633016}. Best is trial 52 with value: 0.020857330411672592.\n[I 2024-05-08 20:09:41,842] Trial 76 finished with value: 0.021009160205721855 and parameters: {'n_estimators': 186, 'max_depth': 6, 'learning_rate': 0.16368720939527426, 'subsample': 0.6448130176989494}. Best is trial 52 with value: 0.020857330411672592.\n[I 2024-05-08 20:10:04,433] Trial 77 finished with value: 0.021019035950303078 and parameters: {'n_estimators': 181, 'max_depth': 5, 'learning_rate': 0.20522499517266124, 'subsample': 0.6053809517241742}. Best is trial 52 with value: 0.020857330411672592.\n[I 2024-05-08 20:10:27,149] Trial 78 finished with value: 0.021473156288266182 and parameters: {'n_estimators': 146, 'max_depth': 6, 'learning_rate': 0.12527583117445099, 'subsample': 0.5198344385025475}. Best is trial 52 with value: 0.020857330411672592.\n[I 2024-05-08 20:10:54,566] Trial 79 finished with value: 0.02112341672182083 and parameters: {'n_estimators': 168, 'max_depth': 7, 'learning_rate': 0.14851985825896194, 'subsample': 0.6878429702441228}. Best is trial 52 with value: 0.020857330411672592.\n[I 2024-05-08 20:11:21,963] Trial 80 finished with value: 0.021036768332123756 and parameters: {'n_estimators': 196, 'max_depth': 6, 'learning_rate': 0.16553921261927332, 'subsample': 0.5577741521890273}. Best is trial 52 with value: 0.020857330411672592.\n[I 2024-05-08 20:11:50,053] Trial 81 finished with value: 0.021004702895879745 and parameters: {'n_estimators': 188, 'max_depth': 6, 'learning_rate': 0.11372185961094013, 'subsample': 0.5679999756194568}. Best is trial 52 with value: 0.020857330411672592.\n[I 2024-05-08 20:12:22,253] Trial 82 finished with value: 0.040507011115550995 and parameters: {'n_estimators': 190, 'max_depth': 5, 'learning_rate': 0.01198085765310667, 'subsample': 0.6110810993445098}. Best is trial 52 with value: 0.020857330411672592.\n[I 2024-05-08 20:12:49,347] Trial 83 finished with value: 0.021136511117219925 and parameters: {'n_estimators': 193, 'max_depth': 6, 'learning_rate': 0.12509290290884814, 'subsample': 0.14821168317711403}. Best is trial 52 with value: 0.020857330411672592.\n[I 2024-05-08 20:13:19,019] Trial 84 finished with value: 0.021156134083867073 and parameters: {'n_estimators': 198, 'max_depth': 6, 'learning_rate': 0.09663469813715024, 'subsample': 0.770689719345969}. Best is trial 52 with value: 0.020857330411672592.\n[I 2024-05-08 20:13:51,395] Trial 85 finished with value: 0.02094917744398117 and parameters: {'n_estimators': 194, 'max_depth': 7, 'learning_rate': 0.10827642631614974, 'subsample': 0.6384707419215978}. Best is trial 52 with value: 0.020857330411672592.\n[I 2024-05-08 20:14:22,449] Trial 86 finished with value: 0.020964764058589935 and parameters: {'n_estimators': 194, 'max_depth': 7, 'learning_rate': 0.10510111675689301, 'subsample': 0.7298917174579296}. Best is trial 52 with value: 0.020857330411672592.\n[I 2024-05-08 20:14:50,774] Trial 87 finished with value: 0.020954599604010582 and parameters: {'n_estimators': 195, 'max_depth': 7, 'learning_rate': 0.10499961115153794, 'subsample': 0.6300517240647389}. Best is trial 52 with value: 0.020857330411672592.\n[I 2024-05-08 20:15:19,927] Trial 88 finished with value: 0.021113146096467972 and parameters: {'n_estimators': 198, 'max_depth': 7, 'learning_rate': 0.08771404771411227, 'subsample': 0.63030132234087}. Best is trial 52 with value: 0.020857330411672592.\n[I 2024-05-08 20:15:53,012] Trial 89 finished with value: 0.021225789561867714 and parameters: {'n_estimators': 200, 'max_depth': 8, 'learning_rate': 0.07261013895089971, 'subsample': 0.5838558314079235}. Best is trial 52 with value: 0.020857330411672592.\n[I 2024-05-08 20:16:18,515] Trial 90 finished with value: 0.021222935989499092 and parameters: {'n_estimators': 195, 'max_depth': 7, 'learning_rate': 0.15815369534296395, 'subsample': 0.5152713998070165}. Best is trial 52 with value: 0.020857330411672592.\n[I 2024-05-08 20:16:46,280] Trial 91 finished with value: 0.02095731906592846 and parameters: {'n_estimators': 194, 'max_depth': 7, 'learning_rate': 0.10614440119210357, 'subsample': 0.6937680601033702}. Best is trial 52 with value: 0.020857330411672592.\n[I 2024-05-08 20:17:13,023] Trial 92 finished with value: 0.0209914892911911 and parameters: {'n_estimators': 193, 'max_depth': 7, 'learning_rate': 0.1330162982653066, 'subsample': 0.6500087236584017}. Best is trial 52 with value: 0.020857330411672592.\n[I 2024-05-08 20:17:42,445] Trial 93 finished with value: 0.021032679826021194 and parameters: {'n_estimators': 186, 'max_depth': 8, 'learning_rate': 0.10808413803060393, 'subsample': 0.6956474103795047}. Best is trial 52 with value: 0.020857330411672592.\n[I 2024-05-08 20:18:11,712] Trial 94 finished with value: 0.02127327397465706 and parameters: {'n_estimators': 196, 'max_depth': 7, 'learning_rate': 0.08097546350516667, 'subsample': 0.6211245726606627}. Best is trial 52 with value: 0.020857330411672592.\n[I 2024-05-08 20:18:34,982] Trial 95 finished with value: 0.021588927134871483 and parameters: {'n_estimators': 198, 'max_depth': 5, 'learning_rate': 0.09809038256552306, 'subsample': 0.6770255805673921}. Best is trial 52 with value: 0.020857330411672592.\n[I 2024-05-08 20:18:53,122] Trial 96 finished with value: 0.020859528332948685 and parameters: {'n_estimators': 189, 'max_depth': 4, 'learning_rate': 0.18889320710862686, 'subsample': 0.7351607638845952}. Best is trial 52 with value: 0.020857330411672592.\n[I 2024-05-08 20:19:09,911] Trial 97 finished with value: 0.02100771851837635 and parameters: {'n_estimators': 183, 'max_depth': 4, 'learning_rate': 0.2606581387744265, 'subsample': 0.7652387975627425}. Best is trial 52 with value: 0.020857330411672592.\n[I 2024-05-08 20:19:25,942] Trial 98 finished with value: 0.0212477408349514 and parameters: {'n_estimators': 189, 'max_depth': 3, 'learning_rate': 0.18571397699098388, 'subsample': 0.7432881005138335}. Best is trial 52 with value: 0.020857330411672592.\n[I 2024-05-08 20:19:44,281] Trial 99 finished with value: 0.02136494219303131 and parameters: {'n_estimators': 178, 'max_depth': 4, 'learning_rate': 0.1485447523286643, 'subsample': 0.47853646496084085}. Best is trial 52 with value: 0.020857330411672592.\n","output_type":"stream"},{"name":"stdout","text":"{'n_estimators': 196, 'max_depth': 5, 'learning_rate': 0.1461774202844157, 'subsample': 0.6649609199174655}\n","output_type":"stream"}]},{"cell_type":"code","source":"xgb_model = xgb.XGBRegressor(n_estimators = 196, max_depth=5, learning_rate=0.1461774202844157, subsample= 0.6649609199174655)\nxgb_model = xgb_model.fit(X_train, y_train)\n\nxgb_preds = tensor(xgb_model.predict(test_dl.xs))\n\nxgb_preds_x = tensor(xgb_model.predict(X_test))\n\nr2_score(y_test,xgb_preds_x)","metadata":{"execution":{"iopub.status.busy":"2024-05-08T20:26:31.840956Z","iopub.execute_input":"2024-05-08T20:26:31.841545Z","iopub.status.idle":"2024-05-08T20:26:59.526206Z","shell.execute_reply.started":"2024-05-08T20:26:31.8415Z","shell.execute_reply":"2024-05-08T20:26:59.525214Z"},"trusted":true},"execution_count":48,"outputs":[{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"0.8330894373473128"},"metadata":{}}]},{"cell_type":"code","source":"def objective_xgboost(trial):\n    params = {\n        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 200),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 1, 10),\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n        \"subsample\": trial.suggest_float(\"subsample\", 0.1, 1.0),\n    }\n    model = xgb.XGBRegressor(**params)\n    model.fit(X_train, y_train)\n    xgb_predictions = model.predict(X_test)\n    r2score = r2_score(y_test, xgb_predictions)\n    return r2score\n\n\nstudy_xgboost = optuna.create_study(direction='minimize')\nstudy_xgboost = optuna_distributed.from_study(study_xgboost)\n\n# Run the optimization with parallel processing\nnum_parallel_jobs = 4 # Adjust this based on your system's capabilities\nstudy_lgbm.optimize(objective_lgbm, n_trials=100, n_jobs=num_parallel_jobs)\n\nprint(study_lgbm.best_params)\nstudy_xgboost.optimize(objective_xgboost, n_trials=30)\nprint(study_xgboost.best_params)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_model = xgb.XGBRegressor(n_estimators = 187, max_depth=8, learning_rate=0.08, subsample=0.7091172175235119)\nxgb_model = xgb_model.fit(X_train, y_train)\n\nxgb_preds = tensor(xgb_model.predict(test_dl.xs))\n\nxgb_preds_x = tensor(xgb_model.predict(X_test))\n\nr2_score(y_test,xgb_preds_x)","metadata":{"execution":{"iopub.status.busy":"2024-05-08T17:31:15.897468Z","iopub.execute_input":"2024-05-08T17:31:15.897826Z","iopub.status.idle":"2024-05-08T17:31:54.3439Z","shell.execute_reply.started":"2024-05-08T17:31:15.897794Z","shell.execute_reply":"2024-05-08T17:31:54.342955Z"},"trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"0.8275227894086742"},"metadata":{}}]},{"cell_type":"markdown","source":"# General Ensemble","metadata":{}},{"cell_type":"code","source":"nn_preds_m = nn_preds_x.squeeze()","metadata":{"execution":{"iopub.status.busy":"2024-05-08T20:35:26.576897Z","iopub.execute_input":"2024-05-08T20:35:26.577441Z","iopub.status.idle":"2024-05-08T20:35:26.583843Z","shell.execute_reply.started":"2024-05-08T20:35:26.5774Z","shell.execute_reply":"2024-05-08T20:35:26.582334Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"#for r2_Score testing\ngeneral_preds = (lgb_preds_x + xgb_preds_x + cat_preds_x + nn_preds_m)/4\ngeneral_preds","metadata":{"execution":{"iopub.status.busy":"2024-05-08T20:35:28.678016Z","iopub.execute_input":"2024-05-08T20:35:28.678457Z","iopub.status.idle":"2024-05-08T20:35:28.69105Z","shell.execute_reply.started":"2024-05-08T20:35:28.678423Z","shell.execute_reply":"2024-05-08T20:35:28.687983Z"},"trusted":true},"execution_count":52,"outputs":[{"execution_count":52,"output_type":"execute_result","data":{"text/plain":"tensor([0.5030, 0.4892, 0.4985,  ..., 0.5667, 0.5888, 0.4134])"},"metadata":{}}]},{"cell_type":"code","source":"#for r2_Score testing\ngeneral_preds = ( cat_preds_x + nn_preds_m)/2\ngeneral_preds","metadata":{"execution":{"iopub.status.busy":"2024-05-08T20:36:30.850293Z","iopub.execute_input":"2024-05-08T20:36:30.85078Z","iopub.status.idle":"2024-05-08T20:36:30.862704Z","shell.execute_reply.started":"2024-05-08T20:36:30.850741Z","shell.execute_reply":"2024-05-08T20:36:30.861301Z"},"trusted":true},"execution_count":54,"outputs":[{"execution_count":54,"output_type":"execute_result","data":{"text/plain":"tensor([0.5058, 0.4884, 0.4997,  ..., 0.5685, 0.5902, 0.4055])"},"metadata":{}}]},{"cell_type":"code","source":"r2_score(y_test,general_preds)","metadata":{"execution":{"iopub.status.busy":"2024-05-08T20:36:37.637252Z","iopub.execute_input":"2024-05-08T20:36:37.638317Z","iopub.status.idle":"2024-05-08T20:36:37.651373Z","shell.execute_reply.started":"2024-05-08T20:36:37.638273Z","shell.execute_reply":"2024-05-08T20:36:37.649647Z"},"trusted":true},"execution_count":55,"outputs":[{"execution_count":55,"output_type":"execute_result","data":{"text/plain":"0.8542809434836184"},"metadata":{}}]},{"cell_type":"code","source":"r2_score(y_test,general_preds)","metadata":{"execution":{"iopub.status.busy":"2024-05-08T20:35:29.24746Z","iopub.execute_input":"2024-05-08T20:35:29.247923Z","iopub.status.idle":"2024-05-08T20:35:29.262853Z","shell.execute_reply.started":"2024-05-08T20:35:29.247875Z","shell.execute_reply":"2024-05-08T20:35:29.261377Z"},"trusted":true},"execution_count":53,"outputs":[{"execution_count":53,"output_type":"execute_result","data":{"text/plain":"0.8507983179178428"},"metadata":{}}]},{"cell_type":"code","source":"!ls","metadata":{"execution":{"iopub.status.busy":"2024-05-08T17:32:33.722202Z","iopub.execute_input":"2024-05-08T17:32:33.722759Z","iopub.status.idle":"2024-05-08T17:32:34.894314Z","shell.execute_reply.started":"2024-05-08T17:32:33.722707Z","shell.execute_reply":"2024-05-08T17:32:34.892679Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"catboost_info  fp_model.pkl  models\n","output_type":"stream"}]},{"cell_type":"code","source":"general_preds.shape","metadata":{"execution":{"iopub.status.busy":"2024-05-05T22:56:37.415324Z","iopub.execute_input":"2024-05-05T22:56:37.415706Z","iopub.status.idle":"2024-05-05T22:56:37.422001Z","shell.execute_reply.started":"2024-05-05T22:56:37.415676Z","shell.execute_reply":"2024-05-05T22:56:37.420818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#use for submission\ngeneral_preds = (cat_preds + nn_preds_y)/2             \ngeneral_preds","metadata":{"execution":{"iopub.status.busy":"2024-05-08T20:41:16.109276Z","iopub.execute_input":"2024-05-08T20:41:16.109955Z","iopub.status.idle":"2024-05-08T20:41:16.126823Z","shell.execute_reply.started":"2024-05-08T20:41:16.109867Z","shell.execute_reply":"2024-05-08T20:41:16.12555Z"},"trusted":true},"execution_count":56,"outputs":[{"execution_count":56,"output_type":"execute_result","data":{"text/plain":"tensor([0.5717, 0.4552, 0.4542,  ..., 0.6213, 0.5530, 0.5111])"},"metadata":{}}]},{"cell_type":"markdown","source":"# Scoring","metadata":{}},{"cell_type":"code","source":"submit = pd.read_csv(path/'sample_submission.csv')\nsubmit['FloodProbability'] = general_preds\nsubmit.to_csv('submission.csv', index=False)\nsub = pd.read_csv('submission.csv')","metadata":{"execution":{"iopub.status.busy":"2024-05-08T20:41:41.291473Z","iopub.execute_input":"2024-05-08T20:41:41.291996Z","iopub.status.idle":"2024-05-08T20:41:43.71969Z","shell.execute_reply.started":"2024-05-08T20:41:41.291951Z","shell.execute_reply":"2024-05-08T20:41:43.718249Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"!ls","metadata":{"execution":{"iopub.status.busy":"2024-05-08T20:41:54.217197Z","iopub.execute_input":"2024-05-08T20:41:54.218706Z","iopub.status.idle":"2024-05-08T20:41:55.469816Z","shell.execute_reply.started":"2024-05-08T20:41:54.218645Z","shell.execute_reply":"2024-05-08T20:41:55.46769Z"},"trusted":true},"execution_count":60,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"catboost_info  fp_model.pkl  models  submission.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"!rm submission.csv","metadata":{"execution":{"iopub.status.busy":"2024-05-05T23:55:36.702687Z","iopub.execute_input":"2024-05-05T23:55:36.703648Z","iopub.status.idle":"2024-05-05T23:55:37.694585Z","shell.execute_reply.started":"2024-05-05T23:55:36.703596Z","shell.execute_reply":"2024-05-05T23:55:37.693193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}